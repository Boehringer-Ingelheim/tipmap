[{"path":"/articles/expert_elicitation.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Specifying a primary weight of the informative prior component","text":"clinical trial project uses Bayesian dynamic borrowing via meta-analytic predictive (MAP) priors, ideally pre-specified weight informative component MAP prior needs determined (Ionan et al. (2023)). Expert elicitation way expert judgement can formally considered statistical inference decision making can used determine weight. reviews expert elicitaton see e.g. Brownstein et al. (2019) O’Hagan (2019), experiences expert elicitation drug development see e.g. Dallow et al. (2018). Sheffield elicitation framework (SHELF) established framework used conduct expert elicitation (O’Hagan (2019), Gosling (2018), Best et al. (2020)), SHELF package available facilitate implementation R (Sheffield Elicitation Framework (2019)). vignette provides brief description expert elicitation can used Bayesian borrowing analysis using robust MAP priors. description expert elicitation process, shows elicited data can processed. based closely resembles functions SHELF package, much limited, sense considered determination one weight parameter (variable scale [0,1]). data example hypothetical data. Loading tipmap package set.seed:","code":"> library(tipmap) > set.seed(123)"},{"path":"/articles/expert_elicitation.html","id":"expert-weightings-collected-using-the-roulette-method","dir":"Articles","previous_headings":"","what":"Expert weightings collected using the roulette method","title":"Specifying a primary weight of the informative prior component","text":", expert data assumed collected via ‘roulette method’ (Gosling (2018), Dallow et al. (2018)). experts asked place 10 chips grid create histogram-like data reflects preferred weighting. particular shape symmetry needed. Data single expert:","code":"> chips_1exp <- c(1, 3, 4, 2, 0, 0, 0, 0, 0, 0) > sum(chips_1exp) # [1] 10"},{"path":"/articles/expert_elicitation.html","id":"fitting-beta-distributions-to-expert-data","dir":"Articles","previous_headings":"","what":"Fitting beta distributions to expert data","title":"Specifying a primary weight of the informative prior component","text":"roulette data assumed follow beta distribution. following calculation fitting beta distribution similar implementation SHELF::fitdist yields identical results. Data single expert: multiple experts individual steps handled fit_beta_mult_exp-function:","code":"> # Compute cumulative probabilities > (x <- get_cum_probs_1exp(chips_1exp)) #  [1] 0.1 0.4 0.8 1.0 1.0 1.0 1.0 1.0 1.0 1.0 > # Compute model inputs > (y <- get_model_input_1exp(x)) #     w cum_probs # 1 0.1       0.1 # 2 0.2       0.4 # 3 0.3       0.8 # 4 0.4       1.0 > # Fit beta distribution > (fit_1exp <- fit_beta_1exp(df = y)$par) #     alpha      beta  #  4.195173 14.182162 > chips_mult <- rbind( +   c(1, 3, 4, 2, 0, 0, 0, 0, 0, 0), +   c(0, 2, 3, 2, 2, 1, 0, 0, 0, 0), +   c(0, 1, 3, 2, 2, 1, 1, 0, 0, 0), +   c(1, 3, 3, 2, 1, 0, 0, 0, 0, 0), +   c(0, 1, 4, 3, 2, 0, 0, 0, 0, 0) + ) > beta_fits <- fit_beta_mult_exp( +   chips_mult = chips_mult + ) > beta_fits #  [38;5;246m# A tibble: 5 × 4 [39m #   alpha  beta    error convergence #    [3m [38;5;246m<dbl> [39m [23m  [3m [38;5;246m<dbl> [39m [23m     [3m [38;5;246m<dbl> [39m [23m  [3m [38;5;246m<lgl> [39m [23m       #  [38;5;250m1 [39m  4.20 14.2  0.001 [4m7 [24m [4m9 [24m  TRUE        #  [38;5;250m2 [39m  3.29  6.97 0.001 [4m3 [24m [4m6 [24m  TRUE        #  [38;5;250m3 [39m  3.30  5.70 0.003 [4m9 [24m [4m9 [24m  TRUE        #  [38;5;250m4 [39m  2.99  9.25 0.000 [4m1 [24m [4m0 [24m [4m6 [24m TRUE        #  [38;5;250m5 [39m  6.90 15.3  0.001 [4m3 [24m [4m6 [24m  TRUE"},{"path":"/articles/expert_elicitation.html","id":"summary-statistics","dir":"Articles","previous_headings":"","what":"Summary statistics","title":"Specifying a primary weight of the informative prior component","text":"Summary statistics single expert: Summary statistics data multiple experts: Mean median values pooled distribution may used primary weights informative component robust MAP prior pre-specifying Bayesian analysis.","code":"> (alpha <- fit_1exp[1]); (beta <- fit_1exp[2]) #    alpha  # 4.195173 #     beta  # 14.18216 >  > # Mean > (beta_mean <- alpha/(alpha+beta)) #     alpha  # 0.2282797 >  > # Standard deviation > beta_sd <- sqrt( (alpha*beta)/( (alpha+beta)^2 *(alpha+beta+1) ) ) > beta_sd #     alpha  # 0.0953491 >  > # Mean absolute deviation around the mean > beta_mad_mean <- (2*(alpha^alpha)*(beta^beta))/( beta(alpha, beta) * (alpha+beta)^(alpha+beta+1) ) > beta_mad_mean #      alpha  # 0.07648378 >  > # Mode > if (alpha > 1 & beta >1) beta_mode <- (alpha-1)/(alpha+beta-2) > if (alpha > 1 & beta >1) beta_mode <- 0.5 > beta_mode # [1] 0.5 >  > # Quantiles > qbeta(p = c(0.001, 0.025, 0.05, 0.1, 0.5, 0.9, 0.95, 0.975, 0.99), +       shape1 = alpha, shape2 = beta) # [1] 0.03030170 0.07286831 0.08986138 0.11249422 0.21827787 0.35769286 0.40104115 # [8] 0.43928844 0.48403929 >  > # Samples > x <- rbeta(n = 10^6, shape1 = alpha, shape2 = beta) > mean(x) # [1] 0.228191 > sd(x) # [1] 0.09536257 > expert_samples <- draw_beta_mixture_nsamples( +   n = 10^3,  +   chips_mult = chips_mult + )  > summary(expert_samples) #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  # 0.03643 0.19375 0.27175 0.28619 0.35947 0.75872 > (mean_w <- round(mean(expert_samples), 2)) # [1] 0.29"},{"path":"/articles/expert_elicitation.html","id":"figures","dir":"Articles","previous_headings":"","what":"Figures","title":"Specifying a primary weight of the informative prior component","text":"","code":"> # Load libraries > packages <- c(\"magrittr\", \"ggplot2\", \"tibble\", \"dplyr\") > invisible(lapply(packages, library, character.only = T))"},{"path":"/articles/expert_elicitation.html","id":"without-linear-pooling","dir":"Articles","previous_headings":"Figures","what":"Without linear pooling","title":"Specifying a primary weight of the informative prior component","text":"","code":"> # Create matrix > fits_mat <- as.matrix(beta_fits[,c(1,2)]) > # Wide format > fit_beta_mult_plot_wide <- tibble::tibble( +  x = seq(0.001, 0.999, length = 200), +  Expert1 = dbeta(x, fits_mat[1,1], fits_mat[1,2]), +  Expert2 = dbeta(x, fits_mat[2,1], fits_mat[2,2]), +  Expert3 = dbeta(x, fits_mat[3,1], fits_mat[3,2]), +  Expert4 = dbeta(x, fits_mat[4,1], fits_mat[4,2]), +  Expert5 = dbeta(x, fits_mat[5,1], fits_mat[5,2]) + ) > # Long format > fit_beta_mult_plot_long <- fit_beta_mult_plot_wide %>% +   tidyr::pivot_longer( +     !x, +     names_to = \"Expert\", +     values_to = \"dens\") > # Plot without linear pool > fig_betas_1 <- ggplot( +   data = fit_beta_mult_plot_long, +   aes(x = x, y = dens, goup = Expert) +   ) + +   geom_line(aes(color = Expert)) + +  ggtitle(\"Fitted beta distributions\") + +  xlab(\"Weight\") + ylab(\"Density\") + +  scale_x_continuous(breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10) / 10) + +  theme_bw() > print(fig_betas_1)"},{"path":"/articles/expert_elicitation.html","id":"with-linear-pooling","dir":"Articles","previous_headings":"Figures","what":"With linear pooling","title":"Specifying a primary weight of the informative prior component","text":"","code":"> # Wide format > fit_beta_mult_plot_wide2 <- fit_beta_mult_plot_wide %>% +   mutate(linpool = (Expert1 + Expert2 + Expert3 + Expert4 + Expert5)/5) > # Long format > fit_beta_mult_plot_long2 <- fit_beta_mult_plot_wide %>% +   tidyr::pivot_longer( +     !x, +     names_to = \"Expert\", +     values_to = \"dens\") > # Plot > fig_betas_2 <- ggplot( +   data = fit_beta_mult_plot_long2, +   aes(x = x, y = dens, group = Expert)) + +   geom_line(aes(color = Expert ) ) + +   ggtitle(\"Fitted beta distributions and linear pool\") + +   xlab(\"Weight\") + ylab(\"Density\") + +   scale_x_continuous(breaks=c(0,1,2,3,4,5,6,7,8,9,10)/10) + +   theme_bw() + +   geom_line(data = fit_beta_mult_plot_wide2, +             aes(x = x, y = linpool, group = 1), +             linewidth=1) > print(fig_betas_2)"},{"path":[]},{"path":"/articles/introduction.html","id":"purpose-of-the-package","dir":"Articles","previous_headings":"","what":"Purpose of the package","title":"Introduction to the 'tipmap' package","text":"R package tipmap implements tipping point analysis clinical trials employ Bayesian dynamic borrowing treatment effect external evidence via robust meta-analytic predictive (MAP) priors. tipping point analysis allows assess much weight informative component robust MAP prior needed conclude investigated treatment efficacious, based total evidence. package mainly provides implementation graphical approach proposed Best et al. (2021) different one-sided evidence levels (80%, 90%, 95%, 97.5%). Tipping point analyses can useful planning analysis stage clinical trial uses external information. planning stage, can help determe (pre-specify) weight informative component MAP prior primary analysis. Various possible results planned trial target population implications treatment effect estimate statistical inferences based total evidence may explored range weights. exercise, addition criteria, decision-makers can develop sense circumstances still feel comfortable establish efficacy specific level certainty. preferred primary weight typically compromise belief applicability data operating characteristics resulting design specifications. analysis stage, tipping point analyses can used sensitivity analysis assess dependency treatment effect estimate statistical inferences weight informative component MAP prior. can also understood sense reverse-Bayes analysis (Held et al. (2022)). vignette shows exemplary application tipping point analysis hypothetical data. functions package (illustrated vignette) facilitate specification robust MAP prior via expert elicitation, specifically choice primary weight (using roulette method). Intended use tipmap-package planning, analysis interpretation (small) clinical trials pediatric drug development, extrapolation efficacy, often Bayesian methods, become increasingly common (Gamalo et al. (2022); ICH (2022); Ionan et al. (2023); Travis et al. (2023)). application, see Maher et al. (2024). implementation MAP prior approach, including computation posterior distribution, RBesT-package used (Weber et al. (2021)).","code":""},{"path":"/articles/introduction.html","id":"generating-data-for-an-exemplary-tipping-point-analysis","dir":"Articles","previous_headings":"","what":"Generating data for an exemplary tipping point analysis","title":"Introduction to the 'tipmap' package","text":"vignette, assume results three clinical trials conducted adult patients (source population) available, share key features new trial among pediatric patients (target population). example, conducted indication, studied drug provided results endpoint interest target population. means certain degree exchangeability trials source target population can assumed. similarity disease response treatment source target population always need carefully considered setting, usually clinical experts disease area. assume supported medical evidence now planned consider trials adult patients Bayesian dynamic borrowing approach, like create robust MAP prior (Schmidli et al. (2014)). treatment effect measure interest assumed mean difference treated group control group respect continuous endpoint.","code":""},{"path":"/articles/introduction.html","id":"derivation-of-map-prior-based-on-trials-in-the-source-population","dir":"Articles","previous_headings":"Generating data for an exemplary tipping point analysis","what":"Derivation of MAP prior based on trials in the source population","title":"Introduction to the 'tipmap' package","text":"start specifying object contains prior data. function create_prior_data() takes vectors total sample sizes, treatment effect estimates standard errors arguments generates data frame. study label optional. generate MAP prior prior data using RBesT-package (Weber et al. (2021)). additional specifications needed made fit MAP prior model; details see Neuenschwander Schmidli (2020) Weber et al. (2021). variable uisd represents assumed unit-information standard deviation specification prior -trial heterogeneity parameter tau follows recommendations consider moderate heterogeneity two-group parameter, mean difference (Neuenschwander Schmidli (2020)). summary fitted model based samples posterior distribution: forest plot Bayesian meta-analysis shown Figure 1. augmented meta-analytic shrinkage estimates per trial. figure shows per-trial point estimates (light point) 95% frequentist confidence intervals (dashed line) model derived median (dark point) 95% credible interval meta-analytic model. Figure 1: Forest plot. Subsequently, MAP prior approximated mixture conjugate normal distributions. parametric form facilitates computation posteriors MAP prior combined results trial target population. approximation yields mixture two normals: density parametric mixture together histogram MCMC samples map_mcmc object shown Figure 2. Figure 2: Overlay MCMC histogram MAP prior fitted parametric mixture approximation. derivation MAP prior now complete. normal likelihoods parametric representation mixture normals can used calculate posterior distributions analytically.","code":"> library(tipmap) > prior_data <- create_prior_data( +   n_total = c(160, 240, 320), +   est = c(1.16, 1.43, 1.59), +   se = c(0.46, 0.35, 0.28) + ) > print(prior_data) #   study_label n_total  est   se # 1     Study 1     160 1.16 0.46 # 2     Study 2     240 1.43 0.35 # 3     Study 3     320 1.59 0.28 > set.seed(123) > uisd <- 5.42 > map_mcmc <- RBesT::gMAP( +   formula = cbind(est, se) ~ 1 | study_label, +   data = prior_data, +   family = gaussian, +   weights = n_total, +   tau.dist = \"HalfNormal\", +   tau.prior = cbind(0, uisd / 16), +   beta.prior = cbind(0, uisd) +   ) > summary(map_mcmc) # Heterogeneity parameter tau per stratum: #         mean    sd    2.5%   50% 97.5% # tau[1] 0.205 0.162 0.00761 0.168 0.603 #  # Regression coefficients: #             mean   sd  2.5%  50% 97.5% # (Intercept) 1.43 0.25 0.922 1.44  1.92 #  # Mean estimate MCMC sample: #            mean   sd  2.5%  50% 97.5% # theta_resp 1.43 0.25 0.922 1.44  1.92 #  # MAP Prior MCMC sample: #                 mean    sd  2.5%  50% 97.5% # theta_resp_pred 1.43 0.356 0.661 1.44   2.1 > plot(map_mcmc)$forest_model > map_prior <- RBesT::automixfit( +   sample = map_mcmc, +   Nc = seq(1, 4), +   k = 6, +   thresh = -Inf +   ) > print(map_prior) # EM for Normal Mixture Model # Log-Likelihood = -1335.201 #  # Univariate normal mixture # Reference scale: 5.298722 # Mixture Components: #   comp1     comp2     # w 0.7717324 0.2282676 # m 1.4522222 1.3625788 # s 0.2508704 0.5793548 > plot(map_prior)$mix"},{"path":"/articles/introduction.html","id":"trial-results-in-the-target-population","dir":"Articles","previous_headings":"Generating data for an exemplary tipping point analysis","what":"Trial results in the target population","title":"Introduction to the 'tipmap' package","text":"now create numeric vector data pediatric trial (total sample size, treatment effect estimate standard error). planning stage, may expected result. function create_new_trial_data() computes quantiles, assuming normally distributed errors. merely used plot confidence interval treatment effect estimate obtained target trial tipping point plot.","code":"> pediatric_trial <- create_new_trial_data(n_total = 30, est = 1.02, se = 1.4) > print(pediatric_trial) #     n_total        mean          se       q0.01      q0.025       q0.05  # 30.00000000  1.02000000  1.40000000 -2.23688702 -1.72394958 -1.28279508  #        q0.1        q0.2       q0.25        q0.5       q0.75        q0.8  # -0.77417219 -0.15826973  0.07571435  1.02000000  1.96428565  2.19826973  #        q0.9       q0.95      q0.975       q0.99  #  2.81417219  3.32279508  3.76394958  4.27688702"},{"path":[]},{"path":"/articles/introduction.html","id":"computation-of-posteriors-for-a-range-of-weights","dir":"Articles","previous_headings":"Performing the tipping point analysis","what":"Computation of posteriors for a range of weights","title":"Introduction to the 'tipmap' package","text":"can now compute posterior distributions range weights using function create_posterior_data(). resulting data frame 201 rows 14 columns. weights increase incrementally steps 0.005 0 1, .e. posterior quantiles 201 weights computed. weight data frame contains following 13 posterior quantiles. posterior quantiles can directly used inferences based total evidence (new data prior combined). reflect one-sided 99%, 97.5%, 95%, 90%, 80%, 50% evidence levels given weight, respectively.","code":"> posterior <- create_posterior_data( +   map_prior = map_prior, +   new_trial_data = pediatric_trial, +   sigma = uisd) > head(posterior, 4) #         weight     q0.01    q0.025     q0.05       q0.1       q0.2      q0.25 # w=0      0.000 -2.197193 -1.700552 -1.273414 -0.7809595 -0.1846242 0.04189379 # w=0.005  0.005 -2.187599 -1.689612 -1.261009 -0.7663718 -0.1663690 0.06195571 # w=0.01   0.010 -2.178066 -1.678733 -1.248665 -0.7518369 -0.1481464 0.08192592 # w=0.015  0.015 -2.168591 -1.667913 -1.236379 -0.7373439 -0.1299534 0.10185010 #              q0.5    q0.75     q0.8     q0.9    q0.95   q0.975    q0.99 # w=0     0.9562020 1.870510 2.097028 2.693363 3.185818 3.612956 4.109597 # w=0.005 0.9830598 1.855910 2.080570 2.678945 3.173427 3.602017 4.100003 # w=0.01  1.0088455 1.842214 2.064425 2.664604 3.161099 3.591139 4.090470 # w=0.015 1.0334528 1.829396 2.048622 2.650338 3.148831 3.580320 4.080995 > tail(posterior, 4) #         weight     q0.01    q0.025     q0.05     q0.1     q0.2    q0.25 # w=0.985  0.985 0.3713404 0.6388480 0.8437628 1.017064 1.174189 1.226884 # w=0.99   0.990 0.3832902 0.6451026 0.8468235 1.018361 1.174756 1.227315 # w=0.995  0.995 0.3949008 0.6512286 0.8498686 1.019639 1.175316 1.227742 # w=1      1.000 0.4061874 0.6572285 0.8528261 1.020899 1.175870 1.228164 #             q0.5    q0.75     q0.8     q0.9    q0.95   q0.975    q0.99 # w=0.985 1.423876 1.613789 1.661732 1.793835 1.916689 2.046120 2.246768 # w=0.99  1.424005 1.613712 1.661591 1.793427 1.915776 2.044136 2.241692 # w=0.995 1.424132 1.613636 1.661451 1.793023 1.914875 2.042187 2.236725 # w=1     1.424259 1.613560 1.661313 1.792624 1.913986 2.040272 2.231867 > colnames(posterior)[-1] #  [1] \"q0.01\"  \"q0.025\" \"q0.05\"  \"q0.1\"   \"q0.2\"   \"q0.25\"  \"q0.5\"   \"q0.75\"  #  [9] \"q0.8\"   \"q0.9\"   \"q0.95\"  \"q0.975\" \"q0.99\""},{"path":"/articles/introduction.html","id":"creating-the-tipping-point-plot","dir":"Articles","previous_headings":"Performing the tipping point analysis","what":"Creating the tipping point plot","title":"Introduction to the 'tipmap' package","text":"function produce tipping point plot called tipmap_plot(), requires dataframe data components generated function create_tipmap_data(). Figure 3: Tipping point plot. center plot, quantiles posterior distribution shown, defining credible intervals effect estimate also one-sided evidence-levels given weights informative component MAP prior. intersections lines connecting respective quantiles horizontal line 0 (null effect) referred tipping points (indicated vertical lines red color). indictae minimum weight required conclude treatment efficacious given one-sided evidence level (Best et al. (2021)). left right side plot, treatment effect estimate obtained trial (pediatric) target population (95% confidence interval) MAP prior (95% credible interval) shown, respectively. plot ggplot-object can modified accordingly. example, chosen primary weight 0.38, add vertical reference line position. additional features customize plot tipmap_plot() function, see help(tipmap_plot). Figure 4: Tipping point plot reference line. see Figure 4 , weight 0.38, probability larger 90% less 95% based posterior distribution treatment effect larger 0, .e. treatment efficacious.","code":"> tipmap_data <- create_tipmap_data( +   new_trial_data = pediatric_trial, +   posterior = posterior, +   map_prior = map_prior) > (p1 <- tipmap_plot(tipmap_data = tipmap_data)) > primary_weight <- 0.38 > (p2 <- p1 + ggplot2::geom_vline(xintercept = primary_weight, col=\"green4\"))"},{"path":"/articles/introduction.html","id":"extracting-quantities-of-interest","dir":"Articles","previous_headings":"Performing the tipping point analysis","what":"Extracting quantities of interest","title":"Introduction to the 'tipmap' package","text":"data frame posteriors weights can filtered obtain posterior quantiles weights specific interest function get_posterior_by_weight(): function get_tipping_points() extracts tipping points one-sided 80%, 90%, 95% 97.5% evidence levels, respectively. Calculating precise posterior probability treatment effect exceeds threshold value possible via functions RBesT-package. posterior probability treatment effect larger 0, 0.5 1, respectively, can assessed cumulative distribution function posterior. illustrated cumulative density curve posterior. Figure 5: Cumulative density posterior weight w=0.38. example, weight corresponding tipping point one-sided evidence-level 95% (=0.51), obtain posterior probability 95% treatment effect larger 0.","code":"> get_posterior_by_weight( +   posterior = posterior,  +   weight = c(primary_weight) +   ) #            q0.01     q0.025      q0.05      q0.1      q0.2    q0.25     q0.5 # w=0.38 -1.532189 -0.9215547 -0.3613189 0.3085641 0.9449431 1.074658 1.386163 #           q0.75     q0.8    q0.9    q0.95  q0.975    q0.99 # w=0.38 1.636538 1.704554 1.94267 2.348325 2.84356 3.444927 > tipp_points <- get_tipping_points( +   tipmap_data = tipmap_data,   +   quantile = c(0.2, 0.1, 0.05, 0.025) + ) > tipp_points #   q0.2   q0.1  q0.05 q0.025  #  0.050  0.275  0.510  0.710 > prior_primary <- RBesT::robustify( +   priormix = map_prior, +   weight = (1 - primary_weight), +   m = 0, +   n = 1, +   sigma = uisd +   ) > posterior_primary <- RBesT::postmix( +   priormix = prior_primary, +   m = pediatric_trial[\"mean\"], +   se = pediatric_trial[\"se\"] +   ) > round(1 - RBesT::pmix(posterior_primary, q = 0), 3) # [1] 0.927 > round(1 - RBesT::pmix(posterior_primary, q = 0.5), 3) # [1] 0.879 > round(1 - RBesT::pmix(posterior_primary, q = 1), 3) # [1] 0.782 > library(ggplot2) > plot(posterior_primary, fun = RBesT::pmix) + +   scale_x_continuous(breaks = seq(-1, 2, 0.5)) + +   scale_y_continuous(breaks = 1-c(1, 0.927, 0.879, 0.782, 0.5, 0), +                      limits = c(0,1), +                      expand = c(0,0) +                      ) + +   ylab(\"Cumulative density of posterior with w=0.38\") + +   xlab(\"Quantile\") + +   geom_segment(aes(x = 0, +                    y = RBesT::pmix(mix = posterior_primary, q = 0),  +                    xend = 0,  +                    yend = 1),  +                col=\"red\") + +   geom_segment(aes(x = 0.5, +                    y = RBesT::pmix(mix = posterior_primary, q = 0.5),  +                    xend = 0.5,  +                    yend = 1),  +                col=\"red\") +  +   geom_segment(aes(x = 1, +                    y = RBesT::pmix(mix = posterior_primary, q = 1),  +                    xend = 1,  +                    yend = 1),  +                col=\"red\") +  +   theme_bw() > tipp_points[3] # q0.05  #  0.51 > prior_95p <- RBesT::robustify( +   priormix = map_prior, +   weight = (1 - tipp_points[3]), +   m = 0, +   n = 1, +   sigma = uisd +   ) > posterior_95p <- RBesT::postmix( +   priormix = prior_95p, +   m = pediatric_trial[\"mean\"], +   se = pediatric_trial[\"se\"] +   ) > round(1 - RBesT::pmix(posterior_95p, q = 0), 3) # [1] 0.95"},{"path":[]},{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Christian Stock. Author, maintainer. Morten Dreher. Author. Emma Torrini. Contributor. Boehringer Ingelheim Pharma GmbH & Co. KG. Copyright holder, funder.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Stock C, Dreher M (2024). tipmap: Tipping Point Analysis Bayesian Dynamic Borrowing. R package version 0.5.3, https://CRAN.R-project.org/package=tipmap.","code":"@Manual{,   title = {tipmap: Tipping Point Analysis for Bayesian Dynamic Borrowing},   author = {Christian Stock and Morten Dreher},   year = {2024},   note = {R package version 0.5.3},   url = {https://CRAN.R-project.org/package=tipmap}, }"},{"path":"/index.html","id":"tipmap-","dir":"","previous_headings":"","what":"Tipping Point Analysis for Bayesian Dynamic Borrowing","title":"Tipping Point Analysis for Bayesian Dynamic Borrowing","text":"tipmap-package facilitates planning analysis partial extrapolation studies pediatric drug development. provides implementation Bayesian tipping point approach can used analyses based robust meta-analytic predictive (MAP) priors. functions facilitate expert elicitation primary (pre-specified) weight informative component MAP prior computation operating characteristics.","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tipping Point Analysis for Bayesian Dynamic Borrowing","text":"CRAN can install current stable version CRAN : GitHub can install current development version GitHub :","code":"install.packages(\"tipmap\") if (!require(\"remotes\")) {install.packages(\"remotes\")} remotes::install_github(\"Boehringer-Ingelheim/tipmap\")"},{"path":"/index.html","id":"getting-started","dir":"","previous_headings":"","what":"Getting started","title":"Tipping Point Analysis for Bayesian Dynamic Borrowing","text":"Load package: prior data (collected source population): data new trial (collected target population): Derivation meta-analytic predictive (MAP) prior: Computing posterior distribution weights informative component MAP prior ranging 0 1: Creating data tipping point analysis (tipping point plot): Create tipping point plot: Get tipping points:","code":"library(tipmap) prior_data <- create_prior_data(   n_total = c(160, 240, 320),   est = c(1.23, 1.40, 1.51),   se = c(0.4, 0.36, 0.31) ) ped_trial <- create_new_trial_data(   n_total = 30,    est = 1.27,    se = 0.95 ) uisd <- sqrt(ped_trial[\"n_total\"]) * ped_trial[\"se\"] g_map <-   RBesT::gMAP(     formula = cbind(est, se) ~ 1 | study_label,     data = prior_data,     family = gaussian,     weights = n_total,     tau.dist = \"HalfNormal\",     tau.prior = cbind(0, uisd / 16),     beta.prior = cbind(0, uisd)   ) map_prior <- RBesT::automixfit(   sample = g_map,   Nc = seq(1, 4),   k = 6,   thresh = -Inf ) posterior <- create_posterior_data(   map_prior = map_prior,   new_trial_data = ped_trial,   sigma = uisd) tipmap_data <- create_tipmap_data(   new_trial_data = ped_trial,   posterior = posterior,   map_prior = map_prior) tipmap_plot(tipmap_data = tipmap_data) get_tipping_points(   tipmap_data,    quantile = c(0.025, 0.05, 0.1, 0.2),    null_effect = 0.1)"},{"path":"/index.html","id":"citing-tipmap","dir":"","previous_headings":"","what":"Citing tipmap","title":"Tipping Point Analysis for Bayesian Dynamic Borrowing","text":"cite tipmap publications please use: Christian Stock Morten Dreher (2023). tipmap: Tipping Point Analysis Bayesian Dynamic Borrowing. R package version 0.5.2. URL: https://CRAN.R-project.org/package=tipmap","code":""},{"path":"/reference/create_new_trial_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Data on new trial in target population — create_new_trial_data","title":"Data on new trial in target population — create_new_trial_data","text":"Creates vector containing data new trial target population. may hypothetical data planning stage.","code":""},{"path":"/reference/create_new_trial_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Data on new trial in target population — create_new_trial_data","text":"","code":"create_new_trial_data(n_total, est, se)"},{"path":"/reference/create_new_trial_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Data on new trial in target population — create_new_trial_data","text":"n_total total sample size. est Treatment effect estimate. se Standard error treatment effect estimate.","code":""},{"path":"/reference/create_new_trial_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Data on new trial in target population — create_new_trial_data","text":"numeric vector data new trial, incl. quantiles assumed normal data likelihood.","code":""},{"path":[]},{"path":"/reference/create_new_trial_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Data on new trial in target population — create_new_trial_data","text":"","code":"new_trial_data <- create_new_trial_data(   n_total = 30, est = 1.27, se = 0.95 )"},{"path":"/reference/create_posterior_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Quantiles of posterior distributions for a range of weights on the \r\ninformative component of the robust MAP prior — create_posterior_data","title":"Quantiles of posterior distributions for a range of weights on the \r\ninformative component of the robust MAP prior — create_posterior_data","text":"Returns data frame containing default quantiles posterior mixture  distributions bounds highest posterior density intervals, generated varying weights informative component MAP  prior.","code":""},{"path":"/reference/create_posterior_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Quantiles of posterior distributions for a range of weights on the \r\ninformative component of the robust MAP prior — create_posterior_data","text":"","code":"create_posterior_data(   map_prior,   new_trial_data,   sigma,   null_effect = 0,   interval_type = \"equal-tailed\",   n_samples = 10000 )"},{"path":"/reference/create_posterior_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Quantiles of posterior distributions for a range of weights on the \r\ninformative component of the robust MAP prior — create_posterior_data","text":"map_prior MAP prior containing information trial(s)  source population, created using RBesT. new_trial_data vector containing information new trial.  See create_new_trial_data(). sigma Standard deviation used weakly informative  component MAP prior, recommended unit-information standard  deviation. null_effect mean robust component MAP prior.  Defaults 0. interval_type type credible interval (character length 1),  either `equal-tailed` (default) `hpdi`, highest posterior density  interval. n_samples Number samples compute highest posterior density  intervals (hence, applicable `interval_type` `hpdi`).","code":""},{"path":"/reference/create_posterior_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Quantiles of posterior distributions for a range of weights on the \r\ninformative component of the robust MAP prior — create_posterior_data","text":"data frame containing default quantiles posterior mixture  distributions bounds highest posterior density intervals.","code":""},{"path":"/reference/create_posterior_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Quantiles of posterior distributions for a range of weights on the \r\ninformative component of the robust MAP prior — create_posterior_data","text":"Highest posterior density intervals based `coda::HPDinterval()` experimental feature.","code":""},{"path":"/reference/create_posterior_data.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Quantiles of posterior distributions for a range of weights on the \r\ninformative component of the robust MAP prior — create_posterior_data","text":"Best, N., Price, R. G., Pouliquen, . J., & Keene, O. N. (2021). Assessing efficacy important subgroups confirmatory trials: example using Bayesian dynamic borrowing. Pharm Stat, 20(3), 551–562. https://doi.org/10.1002/pst.2093","code":""},{"path":[]},{"path":"/reference/create_posterior_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Quantiles of posterior distributions for a range of weights on the \r\ninformative component of the robust MAP prior — create_posterior_data","text":"","code":"# create vector containing data on new trial new_trial_data <- create_new_trial_data(   n_total = 30,   est = 1.27,   se = 0.95 )  # read MAP prior created by RBesT map_prior <- load_tipmap_data(\"tipmapPrior.rds\")  # create posterior data - with interval_type = equal_tailed # (the default for tipping point plots) posterior_data1 <- create_posterior_data(   map_prior = map_prior,   new_trial_data = new_trial_data,   sigma = 12,   interval_type = \"equal-tailed\"    )  # \\donttest{ # create posterior data - with interval_type = hpdi posterior_data2 <- create_posterior_data(   map_prior = map_prior,   new_trial_data = new_trial_data,   sigma = 12,   interval_type = \"hpdi\",   n_samples = 1e4 ) # }"},{"path":"/reference/create_prior_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates input data frame for construction of MAP prior — create_prior_data","title":"Creates input data frame for construction of MAP prior — create_prior_data","text":"Assembling information trials source population structured way (required pre-processing step MAP prior creation).","code":""},{"path":"/reference/create_prior_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates input data frame for construction of MAP prior — create_prior_data","text":"","code":"create_prior_data(study_label = NULL, n_total, est, se)"},{"path":"/reference/create_prior_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates input data frame for construction of MAP prior — create_prior_data","text":"study_label optional vector containing trial labels. n_total vector containing total sample sizes. est vector containing treatment effect estimates. se vector containing standard errors effect estimates.","code":""},{"path":"/reference/create_prior_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates input data frame for construction of MAP prior — create_prior_data","text":"data frame containing data trials source population.","code":""},{"path":[]},{"path":"/reference/create_prior_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creates input data frame for construction of MAP prior — create_prior_data","text":"","code":"prior_data <- create_prior_data(   n_total = c(160, 240, 320),   est = c(1.23, 1.40, 1.51),   se = c(0.4, 0.36, 0.31) )"},{"path":"/reference/create_tipmap_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Create data frame ready to use for tipping point analysis — create_tipmap_data","title":"Create data frame ready to use for tipping point analysis — create_tipmap_data","text":"Combines new trial data created createTargetData(), posterior distribution created create_posterior_data() robust MAP prior using RBesT::automixfit() optional meta-analysis, e.g. created using meta package, data frame needed functions tipmap_plot() get_tipping_point().","code":""},{"path":"/reference/create_tipmap_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create data frame ready to use for tipping point analysis — create_tipmap_data","text":"","code":"create_tipmap_data(new_trial_data, posterior, map_prior, meta_analysis = NULL)"},{"path":"/reference/create_tipmap_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create data frame ready to use for tipping point analysis — create_tipmap_data","text":"new_trial_data data frame containing data new trial target population. See create_new_trial_data(). posterior mixture combining MAP prior target population. See create_posterior_data(). map_prior robust MAP prior created RBesT::automixfit(). meta_analysis data frame containing meta-analysis trial(s) borrowed . See createPriorData().","code":""},{"path":"/reference/create_tipmap_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create data frame ready to use for tipping point analysis — create_tipmap_data","text":"data frame ready used tipmap_plot() get_tipping_point()","code":""},{"path":[]},{"path":"/reference/create_tipmap_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create data frame ready to use for tipping point analysis — create_tipmap_data","text":"","code":"# specify new trial data new_trial_data <- create_new_trial_data(n_total = 30, est = 1.5, se = 2.1)  # read MAP prior data map_prior <- load_tipmap_data(\"tipmapPrior.rds\")  # read posterior data posterior <- load_tipmap_data(\"tipPost.rds\")  tip_dat <- create_tipmap_data(   new_trial_data = new_trial_data,   posterior = posterior,   map_prior = map_prior )"},{"path":"/reference/default_quantiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Default quantiles — default_quantiles","title":"Default quantiles — default_quantiles","text":"Default quantiles","code":""},{"path":"/reference/default_quantiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default quantiles — default_quantiles","text":"","code":"default_quantiles"},{"path":"/reference/default_quantiles.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Default quantiles — default_quantiles","text":"object class numeric length 13.","code":""},{"path":"/reference/default_weights.html","id":null,"dir":"Reference","previous_headings":"","what":"Default weights — default_weights","title":"Default weights — default_weights","text":"Default weights","code":""},{"path":"/reference/default_weights.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default weights — default_weights","text":"","code":"default_weights"},{"path":"/reference/default_weights.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Default weights — default_weights","text":"object class numeric length 201.","code":""},{"path":"/reference/draw_beta_mixture_nsamples.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw samples from a mixture of beta distributions — draw_beta_mixture_nsamples","title":"Draw samples from a mixture of beta distributions — draw_beta_mixture_nsamples","text":"Draws samples mixture beta distributions, representing pooled weights informative component robust MAP prior, elicited experts via roulette method.","code":""},{"path":"/reference/draw_beta_mixture_nsamples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw samples from a mixture of beta distributions — draw_beta_mixture_nsamples","text":"","code":"draw_beta_mixture_nsamples(n, chips_mult, expert_weight = NULL)"},{"path":"/reference/draw_beta_mixture_nsamples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw samples from a mixture of beta distributions — draw_beta_mixture_nsamples","text":"n Numeric value, number samples drawn. chips_mult Numeric matrix, containing expert weighting (distributions chips). Rows represent experts, columns represent bins / weight intervals. expert_weight optional numeric vector, containing weight assigned expert (defaults equal weights).","code":""},{"path":"/reference/draw_beta_mixture_nsamples.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draw samples from a mixture of beta distributions — draw_beta_mixture_nsamples","text":"numeric vector containing samples pooled distribution expert opinions.","code":""},{"path":[]},{"path":"/reference/draw_beta_mixture_nsamples.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Draw samples from a mixture of beta distributions — draw_beta_mixture_nsamples","text":"","code":"rweights <- draw_beta_mixture_nsamples(   n = 50,   chips_mult = rbind(     c(0, 0, 0, 0, 2, 3, 3, 2, 0, 0),     c(0, 0, 0, 1, 2, 4, 2, 1, 0, 0),     c(0, 0, 0, 2, 2, 2, 2, 2, 0, 0)   ),   expert_weight = rep(1/3, 3) ) print(rweights) #>  [1] 0.4532531 0.7611390 0.6710187 0.5180479 0.3724865 0.7165627 0.3533947 #>  [8] 0.5385980 0.4038491 0.5392646 0.6352801 0.6848325 0.5172164 0.6478042 #> [15] 0.6772559 0.5672220 0.7347775 0.6319387 0.2972157 0.4790934 0.5454396 #> [22] 0.6834785 0.5110827 0.3435028 0.7633274 0.6548345 0.5174965 0.5732120 #> [29] 0.6188461 0.4895334 0.5506100 0.6199800 0.5011571 0.7468842 0.6682249 #> [36] 0.4375842 0.7134283 0.4279485 0.6202297 0.5849121 0.5502769 0.5734508 #> [43] 0.6577851 0.6407890 0.4412848 0.5221376 0.7119973 0.3862226 0.6266921 #> [50] 0.3759814 if (FALSE) { hist(rweights) }"},{"path":"/reference/fit_beta_1exp.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit beta distribution for one expert — fit_beta_1exp","title":"Fit beta distribution for one expert — fit_beta_1exp","text":"Fit beta distribution data elicited one expert via roulette method.","code":""},{"path":"/reference/fit_beta_1exp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit beta distribution for one expert — fit_beta_1exp","text":"","code":"fit_beta_1exp(df)"},{"path":"/reference/fit_beta_1exp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit beta distribution for one expert — fit_beta_1exp","text":"df dataframe generated get_model_input_1exp.","code":""},{"path":"/reference/fit_beta_1exp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit beta distribution for one expert — fit_beta_1exp","text":"Parameters (alpha beta) beta fit.","code":""},{"path":"/reference/fit_beta_1exp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit beta distribution for one expert — fit_beta_1exp","text":"function based SHELF::fitdist yields identical results.","code":""},{"path":[]},{"path":"/reference/fit_beta_1exp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit beta distribution for one expert — fit_beta_1exp","text":"","code":"chips <- c(0, 2, 3, 2, 1, 1, 1, 0, 0, 0) x <- get_cum_probs_1exp(chips) y <- get_model_input_1exp(x) fit_beta_1exp(df = y)[\"par\"]  #> $par #>    alpha     beta  #> 2.314860 4.640069  #>"},{"path":"/reference/fit_beta_mult_exp.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit beta distributions for multiple experts — fit_beta_mult_exp","title":"Fit beta distributions for multiple experts — fit_beta_mult_exp","text":"Fit beta distributions data elicited multiple experts via roulette method.","code":""},{"path":"/reference/fit_beta_mult_exp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit beta distributions for multiple experts — fit_beta_mult_exp","text":"","code":"fit_beta_mult_exp(chips_mult)"},{"path":"/reference/fit_beta_mult_exp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit beta distributions for multiple experts — fit_beta_mult_exp","text":"chips_mult dataframe matrix containing weights. contain one row per expert 10 columns, one bin, representing weights 0 1.","code":""},{"path":"/reference/fit_beta_mult_exp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit beta distributions for multiple experts — fit_beta_mult_exp","text":"dataframe containing parameters individual beta distributions.","code":""},{"path":[]},{"path":"/reference/fit_beta_mult_exp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit beta distributions for multiple experts — fit_beta_mult_exp","text":"","code":"beta_fits <- fit_beta_mult_exp(   chips_mult = rbind(     c(0, 0, 0, 0, 2, 3, 3, 2, 0, 0),     c(0, 0, 0, 1, 2, 4, 2, 1, 0, 0),     c(0, 0, 0, 2, 2, 2, 2, 2, 0, 0)   ) ) print(beta_fits) #> # A tibble: 3 × 4 #>   alpha  beta     error convergence #>   <dbl> <dbl>     <dbl> <lgl>       #> 1 10.2   6.87 0.0000466 TRUE        #> 2 11.5   9.46 0.00192   TRUE        #> 3  4.22  3.49 0.000138  TRUE"},{"path":"/reference/get_cum_probs_1exp.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cumulative probabilities from distribution of chips of one expert — get_cum_probs_1exp","title":"Get cumulative probabilities from distribution of chips of one expert — get_cum_probs_1exp","text":"Get cumulative probabilities distribution chips one expert","code":""},{"path":"/reference/get_cum_probs_1exp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cumulative probabilities from distribution of chips of one expert — get_cum_probs_1exp","text":"","code":"get_cum_probs_1exp(chips)"},{"path":"/reference/get_cum_probs_1exp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get cumulative probabilities from distribution of chips of one expert — get_cum_probs_1exp","text":"chips Vector integers, representing distribution chips assigned one expert, elicited roulette method. element vector represents one bin grid.","code":""},{"path":"/reference/get_cum_probs_1exp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get cumulative probabilities from distribution of chips of one expert — get_cum_probs_1exp","text":"numeric vector cumulative distribution chips.","code":""},{"path":[]},{"path":"/reference/get_cum_probs_1exp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get cumulative probabilities from distribution of chips of one expert — get_cum_probs_1exp","text":"","code":"chips <- c(0, 2, 3, 2, 1, 1, 1, 0, 0, 0) x <- get_cum_probs_1exp(chips) print(x) #>  [1] 0.0 0.2 0.5 0.7 0.8 0.9 1.0 1.0 1.0 1.0"},{"path":"/reference/get_model_input_1exp.html","id":null,"dir":"Reference","previous_headings":"","what":"Transform cumulative probabilities to fit beta distributions — get_model_input_1exp","title":"Transform cumulative probabilities to fit beta distributions — get_model_input_1exp","text":"Transform cumulative probabilities fit beta distributions","code":""},{"path":"/reference/get_model_input_1exp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Transform cumulative probabilities to fit beta distributions — get_model_input_1exp","text":"","code":"get_model_input_1exp(cum_probs, w = NULL)"},{"path":"/reference/get_model_input_1exp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Transform cumulative probabilities to fit beta distributions — get_model_input_1exp","text":"cum_probs Numeric vector, containing cumulative probabilities weights one expert, elicited roulette method. element vector represents one bin grid. w Numeric vector, upper interval limit bin (defaults 1:length(cum_probs) / length(cum_probs)).","code":""},{"path":"/reference/get_model_input_1exp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Transform cumulative probabilities to fit beta distributions — get_model_input_1exp","text":"Dataframe used input fit beta distributions [fit_beta_1exp()].","code":""},{"path":[]},{"path":"/reference/get_model_input_1exp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Transform cumulative probabilities to fit beta distributions — get_model_input_1exp","text":"","code":"chips <- c(0, 2, 3, 2, 1, 1, 1, 0, 0, 0) x <- get_cum_probs_1exp(chips) print(x) #>  [1] 0.0 0.2 0.5 0.7 0.8 0.9 1.0 1.0 1.0 1.0 y <- get_model_input_1exp(x) print(y) #>     w cum_probs #> 1 0.2       0.2 #> 2 0.3       0.5 #> 3 0.4       0.7 #> 4 0.5       0.8 #> 5 0.6       0.9 #> 6 0.7       1.0"},{"path":"/reference/get_posterior_by_weight.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter posterior by given weights — get_posterior_by_weight","title":"Filter posterior by given weights — get_posterior_by_weight","text":"Returns quantiles posterior distribution treatment effect one specified weights.","code":""},{"path":"/reference/get_posterior_by_weight.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter posterior by given weights — get_posterior_by_weight","text":"","code":"get_posterior_by_weight(posterior, weight)"},{"path":"/reference/get_posterior_by_weight.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter posterior by given weights — get_posterior_by_weight","text":"posterior posterior data filtered (see [create_posterior_data()]). weight weight(s) filtered .","code":""},{"path":"/reference/get_posterior_by_weight.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter posterior by given weights — get_posterior_by_weight","text":"filtered posterior values","code":""},{"path":[]},{"path":"/reference/get_posterior_by_weight.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter posterior by given weights — get_posterior_by_weight","text":"","code":"get_posterior_by_weight(   posterior = load_tipmap_data(\"tipPost.rds\"),   weight = c(0.05, 0.1) ) #>            q0.01  q0.025     q0.05       q0.1       q0.2     q0.25     q0.5 #> w=0.05 -3.026944 -2.2752 -1.622075 -0.8592616 0.07084512 0.4121273 1.448454 #> w=0.1  -2.847352 -2.0679 -1.384287 -0.5786408 0.37040761 0.6759995 1.476666 #>           q0.75     q0.8     q0.9    q0.95   q0.975    q0.99 #> w=0.05 2.372519 2.685386 3.589860 4.348482 5.000346 5.751503 #> w=0.1  2.193930 2.449017 3.323374 4.114635 4.794578 5.572358"},{"path":"/reference/get_summary_mult_exp.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize expert weights — get_summary_mult_exp","title":"Summarize expert weights — get_summary_mult_exp","text":"Computes minimum, maximum, mean quartiles expert weights.","code":""},{"path":"/reference/get_summary_mult_exp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize expert weights — get_summary_mult_exp","text":"","code":"get_summary_mult_exp(chips_mult, n = 500, expert_weight = NULL)"},{"path":"/reference/get_summary_mult_exp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize expert weights — get_summary_mult_exp","text":"chips_mult Numeric matrix, containing expert weights. n Number samples drawn obtain summary statistics (defaults 500). expert_weight Weights assigned expert (defaults equal weights).","code":""},{"path":"/reference/get_summary_mult_exp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize expert weights — get_summary_mult_exp","text":"vector containing summary statistics.","code":""},{"path":"/reference/get_summary_mult_exp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize expert weights — get_summary_mult_exp","text":"","code":"get_summary_mult_exp(   chips_mult = rbind(     c(0, 0, 0, 0, 2, 3, 3, 2, 0, 0),     c(0, 0, 0, 1, 2, 4, 2, 1, 0, 0),     c(0, 0, 0, 2, 2, 2, 2, 2, 0, 0)   ),    n = 50 ) #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>  0.2598  0.4953  0.5719  0.5712  0.6779  0.8076"},{"path":"/reference/get_tipping_points.html","id":null,"dir":"Reference","previous_headings":"","what":"Identify tipping point for a specific quantile. — get_tipping_points","title":"Identify tipping point for a specific quantile. — get_tipping_points","text":"Identifies weights closest tipping points specified quantiles.","code":""},{"path":"/reference/get_tipping_points.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Identify tipping point for a specific quantile. — get_tipping_points","text":"","code":"get_tipping_points(tipmap_data, quantile, null_effect = 0)"},{"path":"/reference/get_tipping_points.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Identify tipping point for a specific quantile. — get_tipping_points","text":"tipmap_data data frame created create_tipmap_data(). quantile quantile(s) tipping point. Possible values 0.025, 0.05, 0.1, 0.2, 0.8, 0.9, 0.95 0.975. null_effect null treatment effect. Defaults 0.","code":""},{"path":"/reference/get_tipping_points.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Identify tipping point for a specific quantile. — get_tipping_points","text":"weight closest tipping point specified quantile","code":""},{"path":[]},{"path":"/reference/get_tipping_points.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Identify tipping point for a specific quantile. — get_tipping_points","text":"","code":"tip_dat <- load_tipmap_data(\"tipdat.rds\")#' get_tipping_points(tip_dat, quantile = 0.025) #> q0.025  #>   0.75  get_tipping_points(tip_dat, quantile = c(0.025, 0.05, 0.1, 0.2), null_effect = 0.1) #> q0.025  q0.05   q0.1   q0.2  #>  0.825  0.520  0.250  0.055"},{"path":"/reference/load_tipmap_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Load exemplary datasets — load_tipmap_data","title":"Load exemplary datasets — load_tipmap_data","text":"Loads one three exemplary datasets package.","code":""},{"path":"/reference/load_tipmap_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load exemplary datasets — load_tipmap_data","text":"","code":"load_tipmap_data(file)"},{"path":"/reference/load_tipmap_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load exemplary datasets — load_tipmap_data","text":"file dataset loaded.","code":""},{"path":"/reference/load_tipmap_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load exemplary datasets — load_tipmap_data","text":"pre-saved dataset.","code":""},{"path":"/reference/load_tipmap_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load exemplary datasets — load_tipmap_data","text":"","code":"load_tipmap_data(file = \"tipdat.rds\") #>       x.at   x.col    t.est      t.0.025       t.0.05       t.0.1        t.0.2 #> 1   -0.150 new.obs 1.400000 -2.519927969 -1.889707254 -1.16310313 -0.283242467 #> 2    0.000    post 1.362162 -2.504435840 -1.882785262 -1.16607495 -0.298178094 #> 3    0.005    post 1.377881 -2.480192760 -1.855332545 -1.13391992 -0.258782780 #> 4    0.010    post 1.391026 -2.456282693 -1.828229677 -1.10213375 -0.219908205 #> 5    0.015    post 1.402123 -2.432689071 -1.801458943 -1.07069389 -0.181561069 #> 6    0.020    post 1.411628 -2.409396362 -1.775003745 -1.03959423 -0.143751643 #> 7    0.025    post 1.419850 -2.386389983 -1.748875126 -1.00881406 -0.106493573 #> 8    0.030    post 1.427028 -2.363656220 -1.722949838 -0.97833953 -0.069803645 #> 9    0.035    post 1.433348 -2.341182159 -1.697356338 -0.94815800 -0.033701519 #> 10   0.040    post 1.438954 -2.318955619 -1.672020727 -0.91825794  0.001790582 #> 11   0.045    post 1.443959 -2.296965098 -1.646930870 -0.88862894  0.036648217 #> 12   0.050    post 1.448454 -2.275199718 -1.622075314 -0.85926164  0.070845122 #> 13   0.055    post 1.452513 -2.253649180 -1.597443231 -0.83014771  0.104353649 #> 14   0.060    post 1.456196 -2.232303718 -1.573024381 -0.80129965  0.137145252 #> 15   0.065    post 1.459552 -2.211154065 -1.548809069 -0.77266817  0.169190555 #> 16   0.070    post 1.462622 -2.190191492 -1.524788112 -0.74427150  0.200461861 #> 17   0.075    post 1.465442 -2.169407418 -1.500952806 -0.71610522  0.230930815 #> 18   0.080    post 1.468040 -2.148793998 -1.477294899 -0.68816584  0.260571078 #> 19   0.085    post 1.470442 -2.128343620 -1.453806563 -0.66045084  0.289358473 #> 20   0.090    post 1.472668 -2.108049020 -1.430480372 -0.63295862  0.317271648 #> 21   0.095    post 1.474737 -2.087903261 -1.407309284 -0.60568852  0.344292678 #> 22   0.100    post 1.476666 -2.067899706 -1.384286619 -0.57864078  0.370407612 #> 23   0.105    post 1.478467 -2.048037487 -1.361406043 -0.55181653  0.395606906 #> 24   0.110    post 1.480214 -2.028307379 -1.338661557 -0.52521779  0.419885725 #> 25   0.115    post 1.481796 -2.008700252 -1.316047554 -0.49884745  0.443244098 #> 26   0.120    post 1.483283 -1.989210605 -1.293559146 -0.47270921  0.465686920 #> 27   0.125    post 1.484682 -1.969772118 -1.271189895 -0.44680755  0.487223796 #> 28   0.130    post 1.486008 -1.950501776 -1.248935869 -0.42114773  0.507868749 #> 29   0.135    post 1.487264 -1.931333649 -1.226792563 -0.39573570  0.527639815 #> 30   0.140    post 1.488453 -1.912263022 -1.204755751 -0.37058221  0.546558555 #> 31   0.145    post 1.489581 -1.893285350 -1.182821484 -0.34570257  0.564649517 #> 32   0.150    post 1.490651 -1.874396247 -1.160979584 -0.32102744  0.581939667 #> 33   0.155    post 1.491669 -1.855591479 -1.139229665 -0.29668756  0.598457835 #> 34   0.160    post 1.492637 -1.836866956 -1.117573583 -0.27263087  0.614234178 #> 35   0.165    post 1.493560 -1.818218726 -1.096069334 -0.24886661  0.629299690 #> 36   0.170    post 1.494441 -1.799642968 -1.074592086 -0.22540413  0.643685761 #> 37   0.175    post 1.495282 -1.781135982 -1.053200398 -0.20225284  0.657423794 #> 38   0.180    post 1.496086 -1.762694191 -1.031892082 -0.17942210  0.670544876 #> 39   0.185    post 1.496855 -1.744314129 -1.010665237 -0.15692108  0.683068056 #> 40   0.190    post 1.497593 -1.725992440 -0.989518256 -0.13475873  0.695030332 #> 41   0.195    post 1.498299 -1.707725874 -0.968449830 -0.11294527  0.706507527 #> 42   0.200    post 1.498978 -1.689511281 -0.947458952 -0.09148469  0.717457073 #> 43   0.205    post 1.499629 -1.671345609 -0.926544925 -0.07039404  0.727932878 #> 44   0.210    post 1.500255 -1.653225899 -0.905707360 -0.04968531  0.737960225 #> 45   0.215    post 1.500858 -1.635149285 -0.884946191 -0.02930750  0.747534511 #> 46   0.220    post 1.501438 -1.617112992 -0.864261670 -0.00933772  0.756754140 #> 47   0.225    post 1.501996 -1.599114328 -0.843654382  0.01024538  0.765583689 #> 48   0.230    post 1.502535 -1.581150689 -0.823125240  0.02943761  0.774024849 #> 49   0.235    post 1.503054 -1.563219555 -0.802677379  0.04823555  0.782154345 #> 50   0.240    post 1.503556 -1.545320227 -0.782328297  0.06663555  0.789968829 #> 51   0.245    post 1.504040 -1.527446509 -0.761995821  0.08463678  0.797463409 #> 52   0.250    post 1.504508 -1.509598290 -0.741805299  0.10223743  0.804669423 #> 53   0.255    post 1.504960 -1.491773363 -0.721699224  0.11943671  0.811602352 #> 54   0.260    post 1.505398 -1.473969605 -0.701681377  0.13623425  0.818276881 #> 55   0.265    post 1.505822 -1.456184974 -0.681755774  0.15262992  0.824706934 #> 56   0.270    post 1.506233 -1.438417512 -0.661926657  0.16868447  0.830905715 #> 57   0.275    post 1.506630 -1.420665344 -0.642198479  0.18427504  0.836885733 #> 58   0.280    post 1.507016 -1.402926681 -0.622576739  0.19949540  0.842597801 #> 59   0.285    post 1.507390 -1.385199817 -0.603063996  0.21433400  0.848175201 #> 60   0.290    post 1.507753 -1.367483139 -0.583667738  0.22879123  0.853546317 #> 61   0.295    post 1.508106 -1.349775122 -0.564398132  0.24287208  0.858732892 #> 62   0.300    post 1.508448 -1.332074335 -0.545238892  0.25658184  0.863746589 #> 63   0.305    post 1.508780 -1.314371491 -0.526218113  0.26993096  0.868595524 #> 64   0.310    post 1.509104 -1.296671047 -0.507332706  0.28291965  0.873287332 #> 65   0.315    post 1.509418 -1.278975752 -0.488587345  0.29555692  0.877829195 #> 66   0.320    post 1.509724 -1.261345423 -0.469986270  0.30784951  0.882227877 #> 67   0.325    post 1.510022 -1.243656901 -0.451594114  0.31980903  0.886489749 #> 68   0.330    post 1.510311 -1.225970262 -0.433298983  0.33144055  0.890620813 #> 69   0.335    post 1.510593 -1.208284685 -0.415192883  0.34275173  0.894626731 #> 70   0.340    post 1.510868 -1.190599492 -0.397260811  0.35375052  0.898512845 #> 71   0.345    post 1.511136 -1.172914156 -0.379509168  0.36444496  0.902284201 #> 72   0.350    post 1.511397 -1.155228305 -0.361944442  0.37484312  0.905945565 #> 73   0.355    post 1.511652 -1.137541730 -0.344573278  0.38497139  0.909562480 #> 74   0.360    post 1.511901 -1.119854391 -0.327402579  0.39480078  0.913017142 #> 75   0.365    post 1.512143 -1.102166427 -0.310378646  0.40435801  0.916374625 #> 76   0.370    post 1.512380 -1.084478161 -0.293631563  0.41365105  0.919638761 #> 77   0.375    post 1.512611 -1.066790109 -0.277082686  0.42268777  0.922819417 #> 78   0.380    post 1.512836 -1.049102986 -0.260744132  0.43147595  0.925918729 #> 79   0.385    post 1.513057 -1.031425517 -0.244627170  0.44002323  0.928936219 #> 80   0.390    post 1.513272 -1.013735246 -0.228735810  0.44833709  0.931875039 #> 81   0.395    post 1.513483 -0.996057441 -0.213073801  0.45642490  0.934738186 #> 82   0.400    post 1.513688 -0.978385548 -0.197642062  0.46429382  0.937528509 #> 83   0.405    post 1.513890 -0.960721429 -0.182446706  0.47195088  0.940248717 #> 84   0.410    post 1.514087 -0.943067187 -0.167489072  0.47940290  0.942901390 #> 85   0.415    post 1.514279 -0.925452388 -0.152770322  0.48665653  0.945488985 #> 86   0.420    post 1.514468 -0.907806409 -0.138290093  0.49371825  0.948013841 #> 87   0.425    post 1.514652 -0.890189653 -0.124051151  0.50059432  0.950478190 #> 88   0.430    post 1.514833 -0.872599988 -0.110054165  0.50729084  0.952884160 #> 89   0.435    post 1.515010 -0.855035584 -0.096299419  0.51381371  0.955233782 #> 90   0.440    post 1.515183 -0.837499084 -0.082786827  0.52016865  0.957529640 #> 91   0.445    post 1.515353 -0.819994023 -0.069515946  0.52636119  0.959772396 #> 92   0.450    post 1.515519 -0.802523949 -0.056485994  0.53239667  0.961964375 #> 93   0.455    post 1.515682 -0.785092258 -0.043695868  0.53828027  0.964107267 #> 94   0.460    post 1.515842 -0.767702063 -0.031129165  0.54401696  0.966202692 #> 95   0.465    post 1.515999 -0.750356009 -0.018814835  0.54961158  0.968252200 #> 96   0.470    post 1.516153 -0.733117055 -0.006735343  0.55506877  0.970257271 #> 97   0.475    post 1.516303 -0.715896695  0.005111516  0.56039302  0.972219328 #> 98   0.480    post 1.516451 -0.698750496  0.016728174  0.56558865  0.974139730 #> 99   0.485    post 1.516596 -0.681678115  0.028117265  0.57065983  0.976019783 #> 100  0.490    post 1.516739 -0.664686206  0.039281606  0.57561058  0.977860737 #> 101  0.495    post 1.516879 -0.647781898  0.050224179  0.58044478  0.979663792 #> 102  0.500    post 1.517016 -0.630972947  0.060948111  0.58516616  0.981430102 #> 103  0.505    post 1.517151 -0.614206923  0.071456658  0.58977831  0.983160771 #> 104  0.510    post 1.517283 -0.597615681  0.081753186  0.59428469  0.984856862 #> 105  0.515    post 1.517413 -0.581122865  0.091841156  0.59868865  0.986519397 #> 106  0.520    post 1.517541 -0.564740261  0.101724110  0.60299339  0.988149357 #> 107  0.525    post 1.517666 -0.548482892  0.111405654  0.60720202  0.989747686 #> 108  0.530    post 1.517790 -0.532357466  0.120889448  0.61131389  0.991315291 #> 109  0.535    post 1.517911 -0.516369625  0.130179192  0.61533925  0.992853047 #> 110  0.540    post 1.518030 -0.500527084  0.139278611  0.61927712  0.994361795 #> 111  0.545    post 1.518147 -0.484835860  0.148191453  0.62313015  0.995842345 #> 112  0.550    post 1.518262 -0.469302453  0.156921470  0.62690090  0.997295478 #> 113  0.555    post 1.518376 -0.453933474  0.165472415  0.63059186  0.998721945 #> 114  0.560    post 1.518487 -0.438727871  0.173848030  0.63420540  1.000123068 #> 115  0.565    post 1.518597 -0.423693526  0.182052043  0.63774383  1.001498977 #> 116  0.570    post 1.518704 -0.408836140  0.190088157  0.64120934  1.002850340 #> 117  0.575    post 1.518810 -0.394160061  0.197960046  0.64460409  1.004177809 #> 118  0.580    post 1.518915 -0.379669274  0.205669535  0.64793011  1.005482010 #> 119  0.585    post 1.519018 -0.365367384  0.213223964  0.65118940  1.006763550 #> 120  0.590    post 1.519119 -0.351257615  0.220624960  0.65438386  1.008023014 #> 121  0.595    post 1.519218 -0.337342797  0.227876030  0.65751534  1.009260965 #> 122  0.600    post 1.519316 -0.323625369  0.234980634  0.66058562  1.010477951 #> 123  0.605    post 1.519413 -0.310107381  0.241942172  0.66359642  1.011674498 #> 124  0.610    post 1.519508 -0.296747267  0.248763993  0.66654939  1.012851118 #> 125  0.615    post 1.519602 -0.283634265  0.255449385  0.66944612  1.014008302 #> 126  0.620    post 1.519694 -0.270724826  0.262001577  0.67228816  1.015146530 #> 127  0.625    post 1.519785 -0.258019487  0.268423734  0.67507701  1.016266262 #> 128  0.630    post 1.519874 -0.245518438  0.274718964  0.67781409  1.017367945 #> 129  0.635    post 1.519962 -0.233221541  0.280890306  0.68050080  1.018452011 #> 130  0.640    post 1.520049 -0.221128347  0.286940740  0.68313848  1.019518881 #> 131  0.645    post 1.520135 -0.209238117  0.292873178  0.68572843  1.020568960 #> 132  0.650    post 1.520219 -0.197549837  0.298690471  0.68827190  1.021602641 #> 133  0.655    post 1.520303 -0.186062242  0.304395403  0.69077010  1.022620304 #> 134  0.660    post 1.520385 -0.174773830  0.309990697  0.69322421  1.023622320 #> 135  0.665    post 1.520466 -0.163682889  0.315479010  0.69563535  1.024609046 #> 136  0.670    post 1.520545 -0.152787510  0.320862936  0.69800462  1.025580830 #> 137  0.675    post 1.520624 -0.142085606  0.326145009  0.70033308  1.026538008 #> 138  0.680    post 1.520702 -0.131574934  0.331327698  0.70262176  1.027480908 #> 139  0.685    post 1.520778 -0.121253108  0.336413413  0.70487165  1.028409846 #> 140  0.690    post 1.520854 -0.111117619  0.341404502  0.70708371  1.029325130 #> 141  0.695    post 1.520928 -0.101165849  0.346303256  0.70925888  1.030227059 #> 142  0.700    post 1.521002 -0.091395086  0.351111905  0.71139804  1.031115924 #> 143  0.705    post 1.521074 -0.081802536  0.355833173  0.71350207  1.031992005 #> 144  0.710    post 1.521146 -0.072385341  0.360474280  0.71557183  1.032855578 #> 145  0.715    post 1.521216 -0.063140586  0.365031165  0.71760813  1.033706907 #> 146  0.720    post 1.521286 -0.054065316  0.369505852  0.71961175  1.034546252 #> 147  0.725    post 1.521355 -0.045156539  0.373900316  0.72158348  1.035373864 #> 148  0.730    post 1.521423 -0.036411244  0.378216483  0.72352406  1.036189988 #> 149  0.735    post 1.521490 -0.027826403  0.382395190  0.72543421  1.036994860 #> 150  0.740    post 1.521556 -0.019398984  0.386560334  0.72731463  1.037788712 #> 151  0.745    post 1.521622 -0.011125954  0.390652658  0.72916600  1.038571769 #> 152  0.750    post 1.521686 -0.002992701  0.394673892  0.73098899  1.039344250 #> 153  0.755    post 1.521750  0.004942974  0.398625722  0.73278423  1.040106367 #> 154  0.760    post 1.521813  0.012786490  0.402534907  0.73455235  1.040858329 #> 155  0.765    post 1.521875  0.020471315  0.406349200  0.73629396  1.041600336 #> 156  0.770    post 1.521936  0.028018279  0.410099309  0.73800963  1.042332585 #> 157  0.775    post 1.521997  0.035428661  0.413786707  0.73969994  1.043055267 #> 158  0.780    post 1.522057  0.042705320  0.417412826  0.74136544  1.043768569 #> 159  0.785    post 1.522116  0.049851078  0.420979059  0.74300667  1.044472672 #> 160  0.790    post 1.522175  0.056868717  0.424486765  0.74462515  1.045167753 #> 161  0.795    post 1.522233  0.063760976  0.427937266  0.74622226  1.045853985 #> 162  0.800    post 1.522290  0.070530551  0.431331847  0.74779649  1.046531536 #> 163  0.805    post 1.522346  0.077180091  0.434671763  0.74934833  1.047200568 #> 164  0.810    post 1.522402  0.083712201  0.437958233  0.75087826  1.047861243 #> 165  0.815    post 1.522457  0.090129435  0.441192445  0.75238672  1.048513715 #> 166  0.820    post 1.522512  0.096434302  0.444375556  0.75387418  1.049158137 #> 167  0.825    post 1.522566  0.102629260  0.447508694  0.75534105  1.049794657 #> 168  0.830    post 1.522619  0.108716719  0.450592954  0.75678777  1.050423419 #> 169  0.835    post 1.522672  0.114699039  0.453629405  0.75821475  1.051044565 #> 170  0.840    post 1.522724  0.120578531  0.456619088  0.75962238  1.051658231 #> 171  0.845    post 1.522775  0.126357454  0.459563015  0.76101105  1.052264553 #> 172  0.850    post 1.522826  0.132038022  0.462462174  0.76238115  1.052863662 #> 173  0.855    post 1.522876  0.137622397  0.465317525  0.76373304  1.053455685 #> 174  0.860    post 1.522926  0.143112693  0.468130006  0.76500604  1.054040747 #> 175  0.865    post 1.522976  0.148510977  0.470900527  0.76632259  1.054618972 #> 176  0.870    post 1.523024  0.153819267  0.473629978  0.76762198  1.055190477 #> 177  0.875    post 1.523072  0.159039535  0.476319223  0.76890454  1.055755380 #> 178  0.880    post 1.523120  0.164173707  0.478969106  0.77017061  1.056313794 #> 179  0.885    post 1.523167  0.169223662  0.481580448  0.77145771  1.056865831 #> 180  0.890    post 1.523214  0.174191236  0.484154049  0.77268979  1.057411598 #> 181  0.895    post 1.523260  0.179062137  0.486690689  0.77390640  1.057951204 #> 182  0.900    post 1.523306  0.183863485  0.489202509  0.77510781  1.058484751 #> 183  0.905    post 1.523351  0.188588560  0.491679948  0.77629431  1.059012340 #> 184  0.910    post 1.523396  0.193299977  0.494059279  0.77746618  1.059534072 #> 185  0.915    post 1.523440  0.197877207  0.496463540  0.77862368  1.060050043 #> 186  0.920    post 1.523484  0.202382796  0.498832636  0.77976707  1.060560349 #> 187  0.925    post 1.523527  0.206818220  0.501167485  0.78089662  1.061065081 #> 188  0.930    post 1.523570  0.211184921  0.503468976  0.78201256  1.061564332 #> 189  0.935    post 1.523613  0.215484311  0.505737980  0.78311514  1.062058189 #> 190  0.940    post 1.523655  0.219717771  0.507975339  0.78420460  1.062546740 #> 191  0.945    post 1.523696  0.223886650  0.510185906  0.78528117  1.063030070 #> 192  0.950    post 1.523738  0.227992268  0.512341149  0.78634508  1.063508262 #> 193  0.955    post 1.523778  0.232035918  0.514505072  0.78739655  1.063981398 #> 194  0.960    post 1.523819  0.236018860  0.516618327  0.78843579  1.064449557 #> 195  0.965    post 1.523859  0.239942330  0.518744553  0.78946302  1.064912819 #> 196  0.970    post 1.523898  0.243807535  0.520778767  0.79047844  1.065371259 #> 197  0.975    post 1.523938  0.247615655  0.522815432  0.79148225  1.065824951 #> 198  0.980    post 1.523977  0.251367843  0.524825785  0.79247465  1.066273971 #> 199  0.985    post 1.524015  0.255065227  0.526810339  0.79345584  1.066718389 #> 200  0.990    post 1.524053  0.258708910  0.528769598  0.79442600  1.067158276 #> 201  0.995    post 1.524091  0.262299968  0.530704056  0.79538532  1.067593701 #> 202  1.000    post 1.524128  0.265839454  0.532614200  0.79633397  1.068024731 #> 203  1.150   prior 1.542227  0.141257814  0.455419445  0.75583887  1.052426458 #>        t.0.8    t.0.9   t.0.95  t.0.975 #> 1   3.083242 3.963103 4.689707 5.319928 #> 2   3.022502 3.890399 4.607110 5.228760 #> 3   2.984904 3.858641 4.579802 5.204574 #> 4   2.948093 3.827294 4.552856 5.180726 #> 5   2.912094 3.796338 4.526256 5.157200 #> 6   2.876937 3.765770 4.500006 5.133980 #> 7   2.842652 3.735574 4.474056 5.111052 #> 8   2.809269 3.705739 4.448344 5.088403 #> 9   2.776820 3.676257 4.422979 5.066019 #> 10  2.745338 3.647120 4.397888 5.043889 #> 11  2.714851 3.618323 4.373059 5.022002 #> 12  2.685386 3.589860 4.348482 5.000346 #> 13  2.656966 3.561728 4.324147 4.978912 #> 14  2.629608 3.533955 4.300044 4.957691 #> 15  2.603326 3.506476 4.276165 4.936673 #> 16  2.578125 3.479323 4.252501 4.915850 #> 17  2.554004 3.452499 4.229044 4.895213 #> 18  2.530896 3.426003 4.205788 4.874756 #> 19  2.508908 3.399840 4.182725 4.854470 #> 20  2.487953 3.374011 4.159849 4.834349 #> 21  2.467996 3.348521 4.137154 4.814386 #> 22  2.449017 3.323374 4.114635 4.794578 #> 23  2.430981 3.298574 4.092287 4.774921 #> 24  2.413852 3.274128 4.070105 4.755403 #> 25  2.397589 3.250040 4.048085 4.736019 #> 26  2.382152 3.226318 4.026222 4.716702 #> 27  2.367500 3.202966 4.004514 4.697570 #> 28  2.353592 3.179990 3.982957 4.678557 #> 29  2.340388 3.157398 3.961548 4.659658 #> 30  2.327847 3.135195 3.940286 4.640870 #> 31  2.315932 3.113387 3.919167 4.622188 #> 32  2.304607 3.091978 3.898190 4.603607 #> 33  2.293836 3.070976 3.877355 4.585126 #> 34  2.283586 3.050395 3.856649 4.566739 #> 35  2.273826 3.030232 3.836083 4.548443 #> 36  2.264527 3.010421 3.815656 4.530235 #> 37  2.255660 2.991089 3.795429 4.512113 #> 38  2.247200 2.972177 3.775281 4.494072 #> 39  2.239122 2.953688 3.755273 4.476110 #> 40  2.231403 2.935622 3.735406 4.458225 #> 41  2.224023 2.917981 3.715680 4.440414 #> 42  2.216961 2.900770 3.696097 4.422673 #> 43  2.210198 2.883956 3.676658 4.405002 #> 44  2.203718 2.867569 3.657366 4.387397 #> 45  2.197504 2.851649 3.638221 4.369857 #> 46  2.191542 2.836106 3.619228 4.352379 #> 47  2.185816 2.820977 3.600387 4.334963 #> 48  2.180315 2.806251 3.581703 4.317605 #> 49  2.175025 2.791922 3.563177 4.300304 #> 50  2.169936 2.777981 3.544813 4.283062 #> 51  2.165036 2.764477 3.526626 4.265871 #> 52  2.160316 2.751311 3.508614 4.248733 #> 53  2.155767 2.738527 3.490708 4.231648 #> 54  2.151379 2.726109 3.473046 4.214613 #> 55  2.147145 2.714051 3.455544 4.197628 #> 56  2.143057 2.702345 3.438225 4.180692 #> 57  2.139108 2.690984 3.421062 4.163805 #> 58  2.135290 2.679900 3.404159 4.146966 #> 59  2.131598 2.669201 3.387403 4.130175 #> 60  2.128026 2.658798 3.370851 4.113431 #> 61  2.124567 2.648700 3.354500 4.096734 #> 62  2.121218 2.638899 3.338353 4.080083 #> 63  2.117976 2.629385 3.322413 4.063480 #> 64  2.114835 2.620150 3.306681 4.046925 #> 65  2.111788 2.611185 3.291158 4.030417 #> 66  2.108831 2.602483 3.275845 4.013951 #> 67  2.105961 2.594034 3.260802 3.997530 #> 68  2.103174 2.585832 3.245914 3.981161 #> 69  2.100466 2.577868 3.231270 3.964905 #> 70  2.097834 2.570135 3.216849 3.948641 #> 71  2.095214 2.562566 3.202652 3.932431 #> 72  2.092725 2.555275 3.188682 3.916276 #> 73  2.090303 2.548180 3.174939 3.900179 #> 74  2.087946 2.541278 3.161426 3.884140 #> 75  2.085650 2.534568 3.148085 3.868161 #> 76  2.083415 2.528043 3.135033 3.852245 #> 77  2.081237 2.521698 3.122187 3.836388 #> 78  2.079114 2.515526 3.109566 3.820604 #> 79  2.077044 2.509520 3.097169 3.804888 #> 80  2.075025 2.503675 3.084994 3.789243 #> 81  2.073056 2.497986 3.073041 3.773672 #> 82  2.071135 2.492447 3.061308 3.758178 #> 83  2.069259 2.487053 3.049795 3.742763 #> 84  2.067428 2.481798 3.038498 3.727431 #> 85  2.065639 2.476679 3.027417 3.712185 #> 86  2.063892 2.471690 3.016551 3.697028 #> 87  2.062184 2.466828 3.005837 3.681947 #> 88  2.060515 2.462087 2.995395 3.667010 #> 89  2.058884 2.457463 2.985144 3.652127 #> 90  2.057288 2.452953 2.975094 3.637385 #> 91  2.055728 2.448553 2.965243 3.622687 #> 92  2.054201 2.444259 2.955586 3.608142 #> 93  2.052707 2.440068 2.946121 3.593702 #> 94  2.051244 2.435976 2.936844 3.579376 #> 95  2.049812 2.431980 2.927752 3.565168 #> 96  2.048410 2.428078 2.918843 3.551077 #> 97  2.047036 2.424264 2.910113 3.537165 #> 98  2.045691 2.420539 2.901560 3.523317 #> 99  2.044373 2.416897 2.893179 3.509626 #> 100 2.043080 2.413337 2.884968 3.496071 #> 101 2.041814 2.409855 2.876923 3.482655 #> 102 2.040572 2.406451 2.869040 3.469383 #> 103 2.039354 2.403120 2.861332 3.456257 #> 104 2.038160 2.399861 2.853767 3.443282 #> 105 2.036988 2.396673 2.846356 3.430403 #> 106 2.035838 2.393551 2.839094 3.417724 #> 107 2.034710 2.390495 2.831978 3.405188 #> 108 2.033603 2.387503 2.825006 3.392805 #> 109 2.032516 2.384573 2.818174 3.380578 #> 110 2.031448 2.381704 2.811478 3.368507 #> 111 2.030400 2.378894 2.804856 3.356595 #> 112 2.029371 2.376139 2.798425 3.344843 #> 113 2.028359 2.373440 2.792122 3.333252 #> 114 2.027366 2.370794 2.785944 3.321822 #> 115 2.026389 2.368199 2.779888 3.310557 #> 116 2.025430 2.365655 2.773951 3.299396 #> 117 2.024487 2.363159 2.768130 3.288462 #> 118 2.023560 2.360711 2.762423 3.277667 #> 119 2.022648 2.358309 2.756826 3.267032 #> 120 2.021752 2.355952 2.751338 3.256557 #> 121 2.020870 2.353639 2.745956 3.246240 #> 122 2.020003 2.351368 2.740677 3.236081 #> 123 2.019150 2.349139 2.735499 3.226079 #> 124 2.018310 2.346950 2.730419 3.216233 #> 125 2.017485 2.344800 2.725435 3.206541 #> 126 2.016672 2.342688 2.720545 3.197003 #> 127 2.015872 2.340614 2.715746 3.187617 #> 128 2.015084 2.338514 2.711037 3.178381 #> 129 2.014309 2.336511 2.706415 3.169294 #> 130 2.013546 2.334543 2.701878 3.160354 #> 131 2.012794 2.332608 2.697424 3.151560 #> 132 2.012054 2.330706 2.693051 3.142910 #> 133 2.011324 2.328835 2.688758 3.134401 #> 134 2.010606 2.326996 2.684541 3.126032 #> 135 2.009899 2.325188 2.680401 3.117802 #> 136 2.009201 2.323409 2.676334 3.109708 #> 137 2.008514 2.321658 2.672339 3.101748 #> 138 2.007837 2.319936 2.668414 3.093919 #> 139 2.007170 2.318242 2.664558 3.086221 #> 140 2.006512 2.316575 2.660768 3.078651 #> 141 2.005863 2.314934 2.657045 3.071207 #> 142 2.005224 2.313318 2.653385 3.063886 #> 143 2.004594 2.311728 2.649788 3.056688 #> 144 2.003972 2.310162 2.646252 3.049609 #> 145 2.003359 2.308620 2.642775 3.042652 #> 146 2.002754 2.307102 2.639357 3.035811 #> 147 2.002158 2.305606 2.635996 3.029083 #> 148 2.001569 2.304133 2.632690 3.022467 #> 149 2.000989 2.302682 2.629439 3.015960 #> 150 2.000416 2.301252 2.626241 3.009560 #> 151 1.999851 2.299843 2.623095 3.003265 #> 152 1.999293 2.298455 2.619999 2.997013 #> 153 1.998743 2.297087 2.616953 2.990923 #> 154 1.998199 2.295738 2.613956 2.984932 #> 155 1.997663 2.294409 2.611007 2.979039 #> 156 1.997134 2.293098 2.608103 2.973242 #> 157 1.996611 2.291806 2.605245 2.967539 #> 158 1.996095 2.290532 2.602416 2.961928 #> 159 1.995586 2.289275 2.599632 2.956407 #> 160 1.995082 2.288036 2.596955 2.950974 #> 161 1.994586 2.286814 2.594260 2.945629 #> 162 1.994095 2.285609 2.591609 2.940368 #> 163 1.993610 2.284419 2.589000 2.935191 #> 164 1.993131 2.283246 2.586432 2.930096 #> 165 1.992658 2.282088 2.583904 2.925081 #> 166 1.992191 2.280946 2.581415 2.920145 #> 167 1.991729 2.279818 2.578963 2.915286 #> 168 1.991273 2.278706 2.576548 2.910502 #> 169 1.990822 2.277607 2.574169 2.905793 #> 170 1.990377 2.276523 2.571825 2.901156 #> 171 1.989936 2.275453 2.569515 2.896591 #> 172 1.989501 2.274397 2.567239 2.892095 #> 173 1.989071 2.273354 2.564935 2.887668 #> 174 1.988644 2.272324 2.562724 2.883308 #> 175 1.988221 2.271306 2.560545 2.879014 #> 176 1.987803 2.270302 2.558397 2.874784 #> 177 1.987390 2.269310 2.556273 2.870618 #> 178 1.986982 2.268330 2.554175 2.866513 #> 179 1.986578 2.267362 2.552106 2.862470 #> 180 1.986179 2.266406 2.550064 2.858486 #> 181 1.985784 2.265461 2.548051 2.854560 #> 182 1.985393 2.264528 2.546064 2.850692 #> 183 1.985007 2.263606 2.544104 2.846880 #> 184 1.984626 2.262694 2.542170 2.843124 #> 185 1.984248 2.261794 2.540261 2.839405 #> 186 1.983874 2.260904 2.538378 2.835797 #> 187 1.983505 2.260024 2.536518 2.832185 #> 188 1.983139 2.259155 2.534683 2.828630 #> 189 1.982839 2.258295 2.532872 2.825131 #> 190 1.982481 2.257446 2.531083 2.821684 #> 191 1.982127 2.256606 2.529318 2.818288 #> 192 1.981777 2.255775 2.527574 2.814942 #> 193 1.981430 2.254954 2.525853 2.811645 #> 194 1.981087 2.254142 2.524153 2.808396 #> 195 1.980748 2.253339 2.522474 2.805132 #> 196 1.980412 2.252545 2.520816 2.801975 #> 197 1.980080 2.251760 2.519178 2.798849 #> 198 1.979751 2.250983 2.517560 2.795763 #> 199 1.979425 2.250215 2.515962 2.792717 #> 200 1.979102 2.249455 2.514383 2.789713 #> 201 1.978783 2.248704 2.512823 2.786748 #> 202 1.978467 2.247960 2.511282 2.783823 #> 203 2.021465 2.325361 2.643607 2.992356 load_tipmap_data(file = \"tipmapPrior.rds\") #> EM for Normal Mixture Model #> Log-Likelihood = -4035.246 #>  #> Univariate normal mixture #> Reference scale: 9.379118 #> Mixture Components: #>   comp1      comp2      comp3      #> w 0.55100461 0.38671404 0.06228135 #> m 1.53427935 1.52258217 1.73451103 #> s 0.43247561 0.78873751 1.52272336 load_tipmap_data(file = \"tipPost.rds\") #>         weight       q0.01       q0.025        q0.05        q0.1         q0.2 #> w=0      0.000 -3.22723846 -2.504435840 -1.882785262 -1.16607495 -0.298178094 #> w=0.005  0.005 -3.20597224 -2.480192760 -1.855332545 -1.13391992 -0.258782780 #> w=0.01   0.010 -3.18501743 -2.456282693 -1.828229677 -1.10213375 -0.219908205 #> w=0.015  0.015 -3.16435879 -2.432689071 -1.801458943 -1.07069389 -0.181561069 #> w=0.02   0.020 -3.14398203 -2.409396362 -1.775003745 -1.03959423 -0.143751643 #> w=0.025  0.025 -3.12387377 -2.386389983 -1.748875126 -1.00881406 -0.106493573 #> w=0.03   0.030 -3.10402140 -2.363656220 -1.722949838 -0.97833953 -0.069803645 #> w=0.035  0.035 -3.08441307 -2.341182159 -1.697356338 -0.94815800 -0.033701519 #> w=0.04   0.040 -3.06503759 -2.318955619 -1.672020727 -0.91825794  0.001790582 #> w=0.045  0.045 -3.04588442 -2.296965098 -1.646930870 -0.88862894  0.036648217 #> w=0.05   0.050 -3.02694356 -2.275199718 -1.622075314 -0.85926164  0.070845122 #> w=0.055  0.055 -3.00820558 -2.253649180 -1.597443231 -0.83014771  0.104353649 #> w=0.06   0.060 -2.98966151 -2.232303718 -1.573024381 -0.80129965  0.137145252 #> w=0.065  0.065 -2.97130287 -2.211154065 -1.548809069 -0.77266817  0.169190555 #> w=0.07   0.070 -2.95312157 -2.190191492 -1.524788112 -0.74427150  0.200461861 #> w=0.075  0.075 -2.93510993 -2.169407418 -1.500952806 -0.71610522  0.230930815 #> w=0.08   0.080 -2.91726063 -2.148793998 -1.477294899 -0.68816584  0.260571078 #> w=0.085  0.085 -2.89956669 -2.128343620 -1.453806563 -0.66045084  0.289358473 #> w=0.09   0.090 -2.88202147 -2.108049020 -1.430480372 -0.63295862  0.317271648 #> w=0.095  0.095 -2.86461858 -2.087903261 -1.407309284 -0.60568852  0.344292678 #> w=0.1    0.100 -2.84735195 -2.067899706 -1.384286619 -0.57864078  0.370407612 #> w=0.105  0.105 -2.83021574 -2.048037487 -1.361406043 -0.55181653  0.395606906 #> w=0.11   0.110 -2.81320437 -2.028307379 -1.338661557 -0.52521779  0.419885725 #> w=0.115  0.115 -2.79632682 -2.008700252 -1.316047554 -0.49884745  0.443244098 #> w=0.12   0.120 -2.77954896 -1.989210605 -1.293559146 -0.47270921  0.465686920 #> w=0.125  0.125 -2.76288043 -1.969772118 -1.271189895 -0.44680755  0.487223796 #> w=0.13   0.130 -2.74631647 -1.950501776 -1.248935869 -0.42114773  0.507868749 #> w=0.135  0.135 -2.72985249 -1.931333649 -1.226792563 -0.39573570  0.527639815 #> w=0.14   0.140 -2.71348409 -1.912263022 -1.204755751 -0.37058221  0.546558555 #> w=0.145  0.145 -2.69720701 -1.893285350 -1.182821484 -0.34570257  0.564649517 #> w=0.15   0.150 -2.68101713 -1.874396247 -1.160979584 -0.32102744  0.581939667 #> w=0.155  0.155 -2.66491048 -1.855591479 -1.139229665 -0.29668756  0.598457835 #> w=0.16   0.160 -2.64888321 -1.836866956 -1.117573583 -0.27263087  0.614234178 #> w=0.165  0.165 -2.63293160 -1.818218726 -1.096069334 -0.24886661  0.629299690 #> w=0.17   0.170 -2.61705205 -1.799642968 -1.074592086 -0.22540413  0.643685761 #> w=0.175  0.175 -2.60124105 -1.781135982 -1.053200398 -0.20225284  0.657423794 #> w=0.18   0.180 -2.58549520 -1.762694191 -1.031892082 -0.17942210  0.670544876 #> w=0.185  0.185 -2.56981119 -1.744314129 -1.010665237 -0.15692108  0.683068056 #> w=0.19   0.190 -2.55418583 -1.725992440 -0.989518256 -0.13475873  0.695030332 #> w=0.195  0.195 -2.53861599 -1.707725874 -0.968449830 -0.11294527  0.706507527 #> w=0.2    0.200 -2.52309861 -1.689511281 -0.947458952 -0.09148469  0.717457073 #> w=0.205  0.205 -2.50763074 -1.671345609 -0.926544925 -0.07039404  0.727932878 #> w=0.21   0.210 -2.49220947 -1.653225899 -0.905707360 -0.04968531  0.737960225 #> w=0.215  0.215 -2.47683198 -1.635149285 -0.884946191 -0.02930750  0.747534511 #> w=0.22   0.220 -2.46149551 -1.617112992 -0.864261670 -0.00933772  0.756754140 #> w=0.225  0.225 -2.44619735 -1.599114328 -0.843654382  0.01024538  0.765583689 #> w=0.23   0.230 -2.43093427 -1.581150689 -0.823125240  0.02943761  0.774024849 #> w=0.235  0.235 -2.41570484 -1.563219555 -0.802677379  0.04823555  0.782154345 #> w=0.24   0.240 -2.40050597 -1.545320227 -0.782328297  0.06663555  0.789968829 #> w=0.245  0.245 -2.38533516 -1.527446509 -0.761995821  0.08463678  0.797463409 #> w=0.25   0.250 -2.37018997 -1.509598290 -0.741805299  0.10223743  0.804669423 #> w=0.255  0.255 -2.35506799 -1.491773363 -0.721699224  0.11943671  0.811602352 #> w=0.26   0.260 -2.33996685 -1.473969605 -0.701681377  0.13623425  0.818276881 #> w=0.265  0.265 -2.32488424 -1.456184974 -0.681755774  0.15262992  0.824706934 #> w=0.27   0.270 -2.30981785 -1.438417512 -0.661926657  0.16868447  0.830905715 #> w=0.275  0.275 -2.29476545 -1.420665344 -0.642198479  0.18427504  0.836885733 #> w=0.28   0.280 -2.27972479 -1.402926681 -0.622576739  0.19949540  0.842597801 #> w=0.285  0.285 -2.26469370 -1.385199817 -0.603063996  0.21433400  0.848175201 #> w=0.29   0.290 -2.24967000 -1.367483139 -0.583667738  0.22879123  0.853546317 #> w=0.295  0.295 -2.23465156 -1.349775122 -0.564398132  0.24287208  0.858732892 #> w=0.3    0.300 -2.21963626 -1.332074335 -0.545238892  0.25658184  0.863746589 #> w=0.305  0.305 -2.20462938 -1.314371491 -0.526218113  0.26993096  0.868595524 #> w=0.31   0.310 -2.18962091 -1.296671047 -0.507332706  0.28291965  0.873287332 #> w=0.315  0.315 -2.17460865 -1.278975752 -0.488587345  0.29555692  0.877829195 #> w=0.32   0.320 -2.15959064 -1.261345423 -0.469986270  0.30784951  0.882227877 #> w=0.325  0.325 -2.14450387 -1.243656901 -0.451594114  0.31980903  0.886489749 #> w=0.33   0.330 -2.12946848 -1.225970262 -0.433298983  0.33144055  0.890620813 #> w=0.335  0.335 -2.11442148 -1.208284685 -0.415192883  0.34275173  0.894626731 #> w=0.34   0.340 -2.09936096 -1.190599492 -0.397260811  0.35375052  0.898512845 #> w=0.345  0.345 -2.08428500 -1.172914156 -0.379509168  0.36444496  0.902284201 #> w=0.35   0.350 -2.06919170 -1.155228305 -0.361944442  0.37484312  0.905945565 #> w=0.355  0.355 -2.05407916 -1.137541730 -0.344573278  0.38497139  0.909562480 #> w=0.36   0.360 -2.03894549 -1.119854391 -0.327402579  0.39480078  0.913017142 #> w=0.365  0.365 -2.02378880 -1.102166427 -0.310378646  0.40435801  0.916374625 #> w=0.37   0.370 -2.00860722 -1.084478161 -0.293631563  0.41365105  0.919638761 #> w=0.375  0.375 -1.99339887 -1.066790109 -0.277082686  0.42268777  0.922819417 #> w=0.38   0.380 -1.97816188 -1.049102986 -0.260744132  0.43147595  0.925918729 #> w=0.385  0.385 -1.96289439 -1.031425517 -0.244627170  0.44002323  0.928936219 #> w=0.39   0.390 -1.94759454 -1.013735246 -0.228735810  0.44833709  0.931875039 #> w=0.395  0.395 -1.93226047 -0.996057441 -0.213073801  0.45642490  0.934738186 #> w=0.4    0.400 -1.91689032 -0.978385548 -0.197642062  0.46429382  0.937528509 #> w=0.405  0.405 -1.90148225 -0.960721429 -0.182446706  0.47195088  0.940248717 #> w=0.41   0.410 -1.88603440 -0.943067187 -0.167489072  0.47940290  0.942901390 #> w=0.415  0.415 -1.87054494 -0.925452388 -0.152770322  0.48665653  0.945488985 #> w=0.42   0.420 -1.85501202 -0.907806409 -0.138290093  0.49371825  0.948013841 #> w=0.425  0.425 -1.83943381 -0.890189653 -0.124051151  0.50059432  0.950478190 #> w=0.43   0.430 -1.82380848 -0.872599988 -0.110054165  0.50729084  0.952884160 #> w=0.435  0.435 -1.80813420 -0.855035584 -0.096299419  0.51381371  0.955233782 #> w=0.44   0.440 -1.79240916 -0.837499084 -0.082786827  0.52016865  0.957529640 #> w=0.445  0.445 -1.77663154 -0.819994023 -0.069515946  0.52636119  0.959772396 #> w=0.45   0.450 -1.76079954 -0.802523949 -0.056485994  0.53239667  0.961964375 #> w=0.455  0.455 -1.74491137 -0.785092258 -0.043695868  0.53828027  0.964107267 #> w=0.46   0.460 -1.72896524 -0.767702063 -0.031129165  0.54401696  0.966202692 #> w=0.465  0.465 -1.71295939 -0.750356009 -0.018814835  0.54961158  0.968252200 #> w=0.47   0.470 -1.69689796 -0.733117055 -0.006735343  0.55506877  0.970257271 #> w=0.475  0.475 -1.68076637 -0.715896695  0.005111516  0.56039302  0.972219328 #> w=0.48   0.480 -1.66457001 -0.698750496  0.016728174  0.56558865  0.974139730 #> w=0.485  0.485 -1.64830716 -0.681678115  0.028117265  0.57065983  0.976019783 #> w=0.49   0.490 -1.63197615 -0.664686206  0.039281606  0.57561058  0.977860737 #> w=0.495  0.495 -1.61557531 -0.647781898  0.050224179  0.58044478  0.979663792 #> w=0.5    0.500 -1.59910303 -0.630972947  0.060948111  0.58516616  0.981430102 #> w=0.505  0.505 -1.58255773 -0.614206923  0.071456658  0.58977831  0.983160771 #> w=0.51   0.510 -1.56593788 -0.597615681  0.081753186  0.59428469  0.984856862 #> w=0.515  0.515 -1.54924200 -0.581122865  0.091841156  0.59868865  0.986519397 #> w=0.52   0.520 -1.53246866 -0.564740261  0.101724110  0.60299339  0.988149357 #> w=0.525  0.525 -1.51561650 -0.548482892  0.111405654  0.60720202  0.989747686 #> w=0.53   0.530 -1.49868423 -0.532357466  0.120889448  0.61131389  0.991315291 #> w=0.535  0.535 -1.48167062 -0.516369625  0.130179192  0.61533925  0.992853047 #> w=0.54   0.540 -1.46457456 -0.500527084  0.139278611  0.61927712  0.994361795 #> w=0.545  0.545 -1.44739501 -0.484835860  0.148191453  0.62313015  0.995842345 #> w=0.55   0.550 -1.43013106 -0.469302453  0.156921470  0.62690090  0.997295478 #> w=0.555  0.555 -1.41277924 -0.453933474  0.165472415  0.63059186  0.998721945 #> w=0.56   0.560 -1.39533139 -0.438727871  0.173848030  0.63420540  1.000123068 #> w=0.565  0.565 -1.37779941 -0.423693526  0.182052043  0.63774383  1.001498977 #> w=0.57   0.570 -1.36024367 -0.408836140  0.190088157  0.64120934  1.002850340 #> w=0.575  0.575 -1.34254166 -0.394160061  0.197960046  0.64460409  1.004177809 #> w=0.58   0.580 -1.32475416 -0.379669274  0.205669535  0.64793011  1.005482010 #> w=0.585  0.585 -1.30688116 -0.365367384  0.213223964  0.65118940  1.006763550 #> w=0.59   0.590 -1.28892290 -0.351257615  0.220624960  0.65438386  1.008023014 #> w=0.595  0.595 -1.27087208 -0.337342797  0.227876030  0.65751534  1.009260965 #> w=0.6    0.600 -1.25274800 -0.323625369  0.234980634  0.66058562  1.010477951 #> w=0.605  0.605 -1.23454023 -0.310107381  0.241942172  0.66359642  1.011674498 #> w=0.61   0.610 -1.21625047 -0.296747267  0.248763993  0.66654939  1.012851118 #> w=0.615  0.615 -1.19788075 -0.283634265  0.255449385  0.66944612  1.014008302 #> w=0.62   0.620 -1.17943346 -0.270724826  0.262001577  0.67228816  1.015146530 #> w=0.625  0.625 -1.16091137 -0.258019487  0.268423734  0.67507701  1.016266262 #> w=0.63   0.630 -1.14231768 -0.245518438  0.274718964  0.67781409  1.017367945 #> w=0.635  0.635 -1.12365603 -0.233221541  0.280890306  0.68050080  1.018452011 #> w=0.64   0.640 -1.10492552 -0.221128347  0.286940740  0.68313848  1.019518881 #> w=0.645  0.645 -1.08616301 -0.209238117  0.292873178  0.68572843  1.020568960 #> w=0.65   0.650 -1.06730908 -0.197549837  0.298690471  0.68827190  1.021602641 #> w=0.655  0.655 -1.04841968 -0.186062242  0.304395403  0.69077010  1.022620304 #> w=0.66   0.660 -1.02949010 -0.174773830  0.309990697  0.69322421  1.023622320 #> w=0.665  0.665 -1.01052474 -0.163682889  0.315479010  0.69563535  1.024609046 #> w=0.67   0.670 -0.99152930 -0.152787510  0.320862936  0.69800462  1.025580830 #> w=0.675  0.675 -0.97250844 -0.142085606  0.326145009  0.70033308  1.026538008 #> w=0.68   0.680 -0.95352605 -0.131574934  0.331327698  0.70262176  1.027480908 #> w=0.685  0.685 -0.93447197 -0.121253108  0.336413413  0.70487165  1.028409846 #> w=0.69   0.690 -0.91545551 -0.111117619  0.341404502  0.70708371  1.029325130 #> w=0.695  0.695 -0.89645883 -0.101165849  0.346303256  0.70925888  1.030227059 #> w=0.7    0.700 -0.87749446 -0.091395086  0.351111905  0.71139804  1.031115924 #> w=0.705  0.705 -0.85857690 -0.081802536  0.355833173  0.71350207  1.031992005 #> w=0.71   0.710 -0.83966264 -0.072385341  0.360474280  0.71557183  1.032855578 #> w=0.715  0.715 -0.82086910 -0.063140586  0.365031165  0.71760813  1.033706907 #> w=0.72   0.720 -0.80213542 -0.054065316  0.369505852  0.71961175  1.034546252 #> w=0.725  0.725 -0.78349226 -0.045156539  0.373900316  0.72158348  1.035373864 #> w=0.73   0.730 -0.76495300 -0.036411244  0.378216483  0.72352406  1.036189988 #> w=0.735  0.735 -0.74653150 -0.027826403  0.382395190  0.72543421  1.036994860 #> w=0.74   0.740 -0.72824244 -0.019398984  0.386560334  0.72731463  1.037788712 #> w=0.745  0.745 -0.71010188 -0.011125954  0.390652658  0.72916600  1.038571769 #> w=0.75   0.750 -0.69206727 -0.002992701  0.394673892  0.73098899  1.039344250 #> w=0.755  0.755 -0.67424423  0.004942974  0.398625722  0.73278423  1.040106367 #> w=0.76   0.760 -0.65658943  0.012786490  0.402534907  0.73455235  1.040858329 #> w=0.765  0.765 -0.63912408  0.020471315  0.406349200  0.73629396  1.041600336 #> w=0.77   0.770 -0.62185878  0.028018279  0.410099309  0.73800963  1.042332585 #> w=0.775  0.775 -0.60480345  0.035428661  0.413786707  0.73969994  1.043055267 #> w=0.78   0.780 -0.58796728  0.042705320  0.417412826  0.74136544  1.043768569 #> w=0.785  0.785 -0.57135866  0.049851078  0.420979059  0.74300667  1.044472672 #> w=0.79   0.790 -0.55498513  0.056868717  0.424486765  0.74462515  1.045167753 #> w=0.795  0.795 -0.53885340  0.063760976  0.427937266  0.74622226  1.045853985 #> w=0.8    0.800 -0.52296927  0.070530551  0.431331847  0.74779649  1.046531536 #> w=0.805  0.805 -0.50733810  0.077180091  0.434671763  0.74934833  1.047200568 #> w=0.81   0.810 -0.49196305  0.083712201  0.437958233  0.75087826  1.047861243 #> w=0.815  0.815 -0.47684785  0.090129435  0.441192445  0.75238672  1.048513715 #> w=0.82   0.820 -0.46199491  0.096434302  0.444375556  0.75387418  1.049158137 #> w=0.825  0.825 -0.44740586  0.102629260  0.447508694  0.75534105  1.049794657 #> w=0.83   0.830 -0.43308866  0.108716719  0.450592954  0.75678777  1.050423419 #> w=0.835  0.835 -0.41903752  0.114699039  0.453629405  0.75821475  1.051044565 #> w=0.84   0.840 -0.40525022  0.120578531  0.456619088  0.75962238  1.051658231 #> w=0.845  0.845 -0.39172555  0.126357454  0.459563015  0.76101105  1.052264553 #> w=0.85   0.850 -0.37840072  0.132038022  0.462462174  0.76238115  1.052863662 #> w=0.855  0.855 -0.36539562  0.137622397  0.465317525  0.76373304  1.053455685 #> w=0.86   0.860 -0.35264658  0.143112693  0.468130006  0.76500604  1.054040747 #> w=0.865  0.865 -0.34015063  0.148510977  0.470900527  0.76632259  1.054618972 #> w=0.87   0.870 -0.32790446  0.153819267  0.473629978  0.76762198  1.055190477 #> w=0.875  0.875 -0.31590450  0.159039535  0.476319223  0.76890454  1.055755380 #> w=0.88   0.880 -0.30414690  0.164173707  0.478969106  0.77017061  1.056313794 #> w=0.885  0.885 -0.29262765  0.169223662  0.481580448  0.77145771  1.056865831 #> w=0.89   0.890 -0.28134254  0.174191236  0.484154049  0.77268979  1.057411598 #> w=0.895  0.895 -0.27028719  0.179062137  0.486690689  0.77390640  1.057951204 #> w=0.9    0.900 -0.25945715  0.183863485  0.489202509  0.77510781  1.058484751 #> w=0.905  0.905 -0.24884783  0.188588560  0.491679948  0.77629431  1.059012340 #> w=0.91   0.910 -0.23845461  0.193299977  0.494059279  0.77746618  1.059534072 #> w=0.915  0.915 -0.22826445  0.197877207  0.496463540  0.77862368  1.060050043 #> w=0.92   0.920 -0.21830624  0.202382796  0.498832636  0.77976707  1.060560349 #> w=0.925  0.925 -0.20852536  0.206818220  0.501167485  0.78089662  1.061065081 #> w=0.93   0.930 -0.19896383  0.211184921  0.503468976  0.78201256  1.061564332 #> w=0.935  0.935 -0.18955003  0.215484311  0.505737980  0.78311514  1.062058189 #> w=0.94   0.940 -0.18037019  0.219717771  0.507975339  0.78420460  1.062546740 #> w=0.945  0.945 -0.17135862  0.223886650  0.510185906  0.78528117  1.063030070 #> w=0.95   0.950 -0.16252612  0.227992268  0.512341149  0.78634508  1.063508262 #> w=0.955  0.955 -0.15386828  0.232035918  0.514505072  0.78739655  1.063981398 #> w=0.96   0.960 -0.14538071  0.236018860  0.516618327  0.78843579  1.064449557 #> w=0.965  0.965 -0.13705913  0.239942330  0.518744553  0.78946302  1.064912819 #> w=0.97   0.970 -0.12889930  0.243807535  0.520778767  0.79047844  1.065371259 #> w=0.975  0.975 -0.12089711  0.247615655  0.522815432  0.79148225  1.065824951 #> w=0.98   0.980 -0.11304849  0.251367843  0.524825785  0.79247465  1.066273971 #> w=0.985  0.985 -0.10534948  0.255065227  0.526810339  0.79345584  1.066718389 #> w=0.99   0.990 -0.09779623  0.258708910  0.528769598  0.79442600  1.067158276 #> w=0.995  0.995 -0.09038495  0.262299968  0.530704056  0.79538532  1.067593701 #> w=1      1.000 -0.08311199  0.265839454  0.532614200  0.79633397  1.068024731 #>              q0.25     q0.5    q0.75     q0.8     q0.9    q0.95   q0.975 #> w=0     0.03155363 1.362162 2.692771 3.022502 3.890399 4.607110 5.228760 #> w=0.005 0.07366232 1.377881 2.654141 2.984904 3.858641 4.579802 5.204574 #> w=0.01  0.11499205 1.391026 2.616853 2.948093 3.827294 4.552856 5.180726 #> w=0.015 0.15551713 1.402123 2.580895 2.912094 3.796338 4.526256 5.157200 #> w=0.02  0.19523749 1.411628 2.546453 2.876937 3.765770 4.500006 5.133980 #> w=0.025 0.23397996 1.419850 2.513519 2.842652 3.735574 4.474056 5.111052 #> w=0.03  0.27175317 1.427028 2.482148 2.809269 3.705739 4.448344 5.088403 #> w=0.035 0.30851625 1.433348 2.452363 2.776820 3.676257 4.422979 5.066019 #> w=0.04  0.34420122 1.438954 2.424173 2.745338 3.647120 4.397888 5.043889 #> w=0.045 0.37875475 1.443959 2.397567 2.714851 3.618323 4.373059 5.022002 #> w=0.05  0.41212728 1.448454 2.372519 2.685386 3.589860 4.348482 5.000346 #> w=0.055 0.44427505 1.452513 2.348976 2.656966 3.561728 4.324147 4.978912 #> w=0.06  0.47516203 1.456196 2.326894 2.629608 3.533955 4.300044 4.957691 #> w=0.065 0.50476159 1.459552 2.306203 2.603326 3.506476 4.276165 4.936673 #> w=0.07  0.53305766 1.462622 2.286830 2.578125 3.479323 4.252501 4.915850 #> w=0.075 0.56004542 1.465442 2.268699 2.554004 3.452499 4.229044 4.895213 #> w=0.08  0.58573131 1.468040 2.251731 2.530896 3.426003 4.205788 4.874756 #> w=0.085 0.61013255 1.470442 2.235849 2.508908 3.399840 4.182725 4.854470 #> w=0.09  0.63327608 1.472668 2.220937 2.487953 3.374011 4.159849 4.834349 #> w=0.095 0.65519730 1.474737 2.206999 2.467996 3.348521 4.137154 4.814386 #> w=0.1   0.67599950 1.476666 2.193930 2.449017 3.323374 4.114635 4.794578 #> w=0.105 0.69560819 1.478467 2.181663 2.430981 3.298574 4.092287 4.774921 #> w=0.11  0.71415593 1.480214 2.170139 2.413852 3.274128 4.070105 4.755403 #> w=0.115 0.73168529 1.481796 2.159301 2.397589 3.250040 4.048085 4.736019 #> w=0.12  0.74824817 1.483283 2.149097 2.382152 3.226318 4.026222 4.716702 #> w=0.125 0.76390083 1.484682 2.139479 2.367500 3.202966 4.004514 4.697570 #> w=0.13  0.77869725 1.486008 2.130403 2.353592 3.179990 3.982957 4.678557 #> w=0.135 0.79269022 1.487264 2.121828 2.340388 3.157398 3.961548 4.659658 #> w=0.14  0.80593049 1.488453 2.113717 2.327847 3.135195 3.940286 4.640870 #> w=0.145 0.81846642 1.489581 2.106037 2.315932 3.113387 3.919167 4.622188 #> w=0.15  0.83034380 1.490651 2.098756 2.304607 3.091978 3.898190 4.603607 #> w=0.155 0.84160575 1.491669 2.091845 2.293836 3.070976 3.877355 4.585126 #> w=0.16  0.85229271 1.492637 2.085279 2.283586 3.050395 3.856649 4.566739 #> w=0.165 0.86244246 1.493560 2.079034 2.273826 3.030232 3.836083 4.548443 #> w=0.17  0.87209021 1.494441 2.073089 2.264527 3.010421 3.815656 4.530235 #> w=0.175 0.88126871 1.495282 2.067422 2.255660 2.991089 3.795429 4.512113 #> w=0.18  0.89000840 1.496086 2.062016 2.247200 2.972177 3.775281 4.494072 #> w=0.185 0.89833747 1.496855 2.056854 2.239122 2.953688 3.755273 4.476110 #> w=0.19  0.90628209 1.497593 2.051919 2.231403 2.935622 3.735406 4.458225 #> w=0.195 0.91386650 1.498299 2.047199 2.224023 2.917981 3.715680 4.440414 #> w=0.2   0.92111314 1.498978 2.042680 2.216961 2.900770 3.696097 4.422673 #> w=0.205 0.92804282 1.499629 2.038349 2.210198 2.883956 3.676658 4.405002 #> w=0.21  0.93467481 1.500255 2.034195 2.203718 2.867569 3.657366 4.387397 #> w=0.215 0.94102697 1.500858 2.030208 2.197504 2.851649 3.638221 4.369857 #> w=0.22  0.94711588 1.501438 2.026379 2.191542 2.836106 3.619228 4.352379 #> w=0.225 0.95295689 1.501996 2.022697 2.185816 2.820977 3.600387 4.334963 #> w=0.23  0.95856428 1.502535 2.019156 2.180315 2.806251 3.581703 4.317605 #> w=0.235 0.96395130 1.503054 2.015748 2.175025 2.791922 3.563177 4.300304 #> w=0.24  0.96913028 1.503556 2.012464 2.169936 2.777981 3.544813 4.283062 #> w=0.245 0.97411267 1.504040 2.009299 2.165036 2.764477 3.526626 4.265871 #> w=0.25  0.97890915 1.504508 2.006246 2.160316 2.751311 3.508614 4.248733 #> w=0.255 0.98352967 1.504960 2.003300 2.155767 2.738527 3.490708 4.231648 #> w=0.26  0.98798351 1.505398 2.000456 2.151379 2.726109 3.473046 4.214613 #> w=0.265 0.99227932 1.505822 1.997707 2.147145 2.714051 3.455544 4.197628 #> w=0.27  0.99642519 1.506233 1.995049 2.143057 2.702345 3.438225 4.180692 #> w=0.275 1.00042869 1.506630 1.992479 2.139108 2.690984 3.421062 4.163805 #> w=0.28  1.00429690 1.507016 1.989991 2.135290 2.679900 3.404159 4.146966 #> w=0.285 1.00803643 1.507390 1.987583 2.131598 2.669201 3.387403 4.130175 #> w=0.29  1.01165351 1.507753 1.985249 2.128026 2.658798 3.370851 4.113431 #> w=0.295 1.01515397 1.508106 1.982988 2.124567 2.648700 3.354500 4.096734 #> w=0.3   1.01854327 1.508448 1.980795 2.121218 2.638899 3.338353 4.080083 #> w=0.305 1.02182657 1.508780 1.978667 2.117976 2.629385 3.322413 4.063480 #> w=0.31  1.02500869 1.509104 1.976603 2.114835 2.620150 3.306681 4.046925 #> w=0.315 1.02809420 1.509418 1.974598 2.111788 2.611185 3.291158 4.030417 #> w=0.32  1.03108737 1.509724 1.972650 2.108831 2.602483 3.275845 4.013951 #> w=0.325 1.03399226 1.510022 1.970758 2.105961 2.594034 3.260802 3.997530 #> w=0.33  1.03681266 1.510311 1.968918 2.103174 2.585832 3.245914 3.981161 #> w=0.335 1.03955220 1.510593 1.967129 2.100466 2.577868 3.231270 3.964905 #> w=0.34  1.04221683 1.510868 1.965389 2.097834 2.570135 3.216849 3.948641 #> w=0.345 1.04483158 1.511136 1.963695 2.095214 2.562566 3.202652 3.932431 #> w=0.35  1.04734626 1.511397 1.962046 2.092725 2.555275 3.188682 3.916276 #> w=0.355 1.04979273 1.511652 1.960439 2.090303 2.548180 3.174939 3.900179 #> w=0.36  1.05217371 1.511901 1.958874 2.087946 2.541278 3.161426 3.884140 #> w=0.365 1.05449179 1.512143 1.957349 2.085650 2.534568 3.148085 3.868161 #> w=0.37  1.05674940 1.512380 1.955863 2.083415 2.528043 3.135033 3.852245 #> w=0.375 1.05894888 1.512611 1.954413 2.081237 2.521698 3.122187 3.836388 #> w=0.38  1.06109241 1.512836 1.952998 2.079114 2.515526 3.109566 3.820604 #> w=0.385 1.06318211 1.513057 1.951618 2.077044 2.509520 3.097169 3.804888 #> w=0.39  1.06521996 1.513272 1.950271 2.075025 2.503675 3.084994 3.789243 #> w=0.395 1.06720786 1.513483 1.948956 2.073056 2.497986 3.073041 3.773672 #> w=0.4   1.06914761 1.513688 1.947671 2.071135 2.492447 3.061308 3.758178 #> w=0.405 1.07104094 1.513890 1.946417 2.069259 2.487053 3.049795 3.742763 #> w=0.41  1.07288949 1.514087 1.945190 2.067428 2.481798 3.038498 3.727431 #> w=0.415 1.07469481 1.514279 1.943992 2.065639 2.476679 3.027417 3.712185 #> w=0.42  1.07645841 1.514468 1.942820 2.063892 2.471690 3.016551 3.697028 #> w=0.425 1.07818171 1.514652 1.941675 2.062184 2.466828 3.005837 3.681947 #> w=0.43  1.07986607 1.514833 1.940554 2.060515 2.462087 2.995395 3.667010 #> w=0.435 1.08151279 1.515010 1.939457 2.058884 2.457463 2.985144 3.652127 #> w=0.44  1.08312311 1.515183 1.938384 2.057288 2.452953 2.975094 3.637385 #> w=0.445 1.08469823 1.515353 1.937334 2.055728 2.448553 2.965243 3.622687 #> w=0.45  1.08623929 1.515519 1.936306 2.054201 2.444259 2.955586 3.608142 #> w=0.455 1.08774737 1.515682 1.935299 2.052707 2.440068 2.946121 3.593702 #> w=0.46  1.08922351 1.515842 1.934312 2.051244 2.435976 2.936844 3.579376 #> w=0.465 1.09066873 1.515999 1.933346 2.049812 2.431980 2.927752 3.565168 #> w=0.47  1.09208398 1.516153 1.932399 2.048410 2.428078 2.918843 3.551077 #> w=0.475 1.09347018 1.516303 1.931471 2.047036 2.424264 2.910113 3.537165 #> w=0.48  1.09482822 1.516451 1.930562 2.045691 2.420539 2.901560 3.523317 #> w=0.485 1.09615894 1.516596 1.929670 2.044373 2.416897 2.893179 3.509626 #> w=0.49  1.09746315 1.516739 1.928795 2.043080 2.413337 2.884968 3.496071 #> w=0.495 1.09874165 1.516879 1.927937 2.041814 2.409855 2.876923 3.482655 #> w=0.5   1.09999518 1.517016 1.927096 2.040572 2.406451 2.869040 3.469383 #> w=0.505 1.10122447 1.517151 1.926270 2.039354 2.403120 2.861332 3.456257 #> w=0.51  1.10243021 1.517283 1.925460 2.038160 2.399861 2.853767 3.443282 #> w=0.515 1.10361307 1.517413 1.924664 2.036988 2.396673 2.846356 3.430403 #> w=0.52  1.10477369 1.517541 1.923883 2.035838 2.393551 2.839094 3.417724 #> w=0.525 1.10591271 1.517666 1.923113 2.034710 2.390495 2.831978 3.405188 #> w=0.53  1.10703070 1.517790 1.922357 2.033603 2.387503 2.825006 3.392805 #> w=0.535 1.10812826 1.517911 1.921614 2.032516 2.384573 2.818174 3.380578 #> w=0.54  1.10920593 1.518030 1.920885 2.031448 2.381704 2.811478 3.368507 #> w=0.545 1.11026425 1.518147 1.920168 2.030400 2.378894 2.804856 3.356595 #> w=0.55  1.11130373 1.518262 1.919464 2.029371 2.376139 2.798425 3.344843 #> w=0.555 1.11232488 1.518376 1.918772 2.028359 2.373440 2.792122 3.333252 #> w=0.56  1.11332818 1.518487 1.918093 2.027366 2.370794 2.785944 3.321822 #> w=0.565 1.11431408 1.518597 1.917424 2.026389 2.368199 2.779888 3.310557 #> w=0.57  1.11528304 1.518704 1.916767 2.025430 2.365655 2.773951 3.299396 #> w=0.575 1.11623549 1.518810 1.916121 2.024487 2.363159 2.768130 3.288462 #> w=0.58  1.11717185 1.518915 1.915547 2.023560 2.360711 2.762423 3.277667 #> w=0.585 1.11809251 1.519018 1.914922 2.022648 2.358309 2.756826 3.267032 #> w=0.59  1.11899789 1.519119 1.914307 2.021752 2.355952 2.751338 3.256557 #> w=0.595 1.11988834 1.519218 1.913702 2.020870 2.353639 2.745956 3.246240 #> w=0.6   1.12076424 1.519316 1.913107 2.020003 2.351368 2.740677 3.236081 #> w=0.605 1.12162593 1.519413 1.912521 2.019150 2.349139 2.735499 3.226079 #> w=0.61  1.12247378 1.519508 1.911945 2.018310 2.346950 2.730419 3.216233 #> w=0.615 1.12330809 1.519602 1.911378 2.017485 2.344800 2.725435 3.206541 #> w=0.62  1.12412920 1.519694 1.910819 2.016672 2.342688 2.720545 3.197003 #> w=0.625 1.12493742 1.519785 1.910269 2.015872 2.340614 2.715746 3.187617 #> w=0.63  1.12573304 1.519874 1.909728 2.015084 2.338514 2.711037 3.178381 #> w=0.635 1.12651637 1.519962 1.909195 2.014309 2.336511 2.706415 3.169294 #> w=0.64  1.12728768 1.520049 1.908670 2.013546 2.334543 2.701878 3.160354 #> w=0.645 1.12804724 1.520135 1.908152 2.012794 2.332608 2.697424 3.151560 #> w=0.65  1.12879533 1.520219 1.907643 2.012054 2.330706 2.693051 3.142910 #> w=0.655 1.12953220 1.520303 1.907140 2.011324 2.328835 2.688758 3.134401 #> w=0.66  1.13025810 1.520385 1.906646 2.010606 2.326996 2.684541 3.126032 #> w=0.665 1.13097327 1.520466 1.906158 2.009899 2.325188 2.680401 3.117802 #> w=0.67  1.13167796 1.520545 1.905678 2.009201 2.323409 2.676334 3.109708 #> w=0.675 1.13237239 1.520624 1.905204 2.008514 2.321658 2.672339 3.101748 #> w=0.68  1.13305678 1.520702 1.904737 2.007837 2.319936 2.668414 3.093919 #> w=0.685 1.13373134 1.520778 1.904277 2.007170 2.318242 2.664558 3.086221 #> w=0.69  1.13439630 1.520854 1.903823 2.006512 2.316575 2.660768 3.078651 #> w=0.695 1.13505185 1.520928 1.903375 2.005863 2.314934 2.657045 3.071207 #> w=0.7   1.13569819 1.521002 1.902934 2.005224 2.313318 2.653385 3.063886 #> w=0.705 1.13633551 1.521074 1.902499 2.004594 2.311728 2.649788 3.056688 #> w=0.71  1.13696401 1.521146 1.902069 2.003972 2.310162 2.646252 3.049609 #> w=0.715 1.13758386 1.521216 1.901646 2.003359 2.308620 2.642775 3.042652 #> w=0.72  1.13819523 1.521286 1.901228 2.002754 2.307102 2.639357 3.035811 #> w=0.725 1.13879831 1.521355 1.900816 2.002158 2.305606 2.635996 3.029083 #> w=0.73  1.13939327 1.521423 1.900409 2.001569 2.304133 2.632690 3.022467 #> w=0.735 1.13998025 1.521490 1.900007 2.000989 2.302682 2.629439 3.015960 #> w=0.74  1.14055943 1.521556 1.899611 2.000416 2.301252 2.626241 3.009560 #> w=0.745 1.14113096 1.521622 1.899220 1.999851 2.299843 2.623095 3.003265 #> w=0.75  1.14169498 1.521686 1.898834 1.999293 2.298455 2.619999 2.997013 #> w=0.755 1.14225165 1.521750 1.898453 1.998743 2.297087 2.616953 2.990923 #> w=0.76  1.14280111 1.521813 1.898077 1.998199 2.295738 2.613956 2.984932 #> w=0.765 1.14334350 1.521875 1.897706 1.997663 2.294409 2.611007 2.979039 #> w=0.77  1.14387895 1.521936 1.897339 1.997134 2.293098 2.608103 2.973242 #> w=0.775 1.14440760 1.521997 1.896977 1.996611 2.291806 2.605245 2.967539 #> w=0.78  1.14492956 1.522057 1.896620 1.996095 2.290532 2.602416 2.961928 #> w=0.785 1.14544498 1.522116 1.896267 1.995586 2.289275 2.599632 2.956407 #> w=0.79  1.14595398 1.522175 1.895918 1.995082 2.288036 2.596955 2.950974 #> w=0.795 1.14645666 1.522233 1.895573 1.994586 2.286814 2.594260 2.945629 #> w=0.8   1.14695316 1.522290 1.895233 1.994095 2.285609 2.591609 2.940368 #> w=0.805 1.14744357 1.522346 1.894897 1.993610 2.284419 2.589000 2.935191 #> w=0.81  1.14792802 1.522402 1.894590 1.993131 2.283246 2.586432 2.930096 #> w=0.815 1.14840661 1.522457 1.894220 1.992658 2.282088 2.583904 2.925081 #> w=0.82  1.14887945 1.522512 1.893929 1.992191 2.280946 2.581415 2.920145 #> w=0.825 1.14934664 1.522566 1.893591 1.991729 2.279818 2.578963 2.915286 #> w=0.83  1.14980828 1.522619 1.893275 1.991273 2.278706 2.576548 2.910502 #> w=0.835 1.15026447 1.522672 1.892962 1.990822 2.277607 2.574169 2.905793 #> w=0.84  1.15071531 1.522724 1.892652 1.990377 2.276523 2.571825 2.901156 #> w=0.845 1.15116088 1.522775 1.892346 1.989936 2.275453 2.569515 2.896591 #> w=0.85  1.15160129 1.522826 1.892044 1.989501 2.274397 2.567239 2.892095 #> w=0.855 1.15203661 1.522876 1.891745 1.989071 2.273354 2.564935 2.887668 #> w=0.86  1.15246695 1.522926 1.891449 1.988644 2.272324 2.562724 2.883308 #> w=0.865 1.15289238 1.522976 1.891157 1.988221 2.271306 2.560545 2.879014 #> w=0.87  1.15331298 1.523024 1.890868 1.987803 2.270302 2.558397 2.874784 #> w=0.875 1.15372884 1.523072 1.890582 1.987390 2.269310 2.556273 2.870618 #> w=0.88  1.15414004 1.523120 1.890300 1.986982 2.268330 2.554175 2.866513 #> w=0.885 1.15454666 1.523167 1.890020 1.986578 2.267362 2.552106 2.862470 #> w=0.89  1.15494877 1.523214 1.889744 1.986179 2.266406 2.550064 2.858486 #> w=0.895 1.15534644 1.523260 1.889470 1.985784 2.265461 2.548051 2.854560 #> w=0.9   1.15573976 1.523306 1.889200 1.985393 2.264528 2.546064 2.850692 #> w=0.905 1.15612878 1.523351 1.888932 1.985007 2.263606 2.544104 2.846880 #> w=0.91  1.15651359 1.523396 1.888667 1.984626 2.262694 2.542170 2.843124 #> w=0.915 1.15689425 1.523440 1.888405 1.984248 2.261794 2.540261 2.839405 #> w=0.92  1.15727082 1.523484 1.888146 1.983874 2.260904 2.538378 2.835797 #> w=0.925 1.15764337 1.523527 1.887890 1.983505 2.260024 2.536518 2.832185 #> w=0.93  1.15801197 1.523570 1.887636 1.983139 2.259155 2.534683 2.828630 #> w=0.935 1.15837668 1.523613 1.887385 1.982839 2.258295 2.532872 2.825131 #> w=0.94  1.15873756 1.523655 1.887137 1.982481 2.257446 2.531083 2.821684 #> w=0.945 1.15909467 1.523696 1.886891 1.982127 2.256606 2.529318 2.818288 #> w=0.95  1.15944806 1.523738 1.886647 1.981777 2.255775 2.527574 2.814942 #> w=0.955 1.15979780 1.523778 1.886406 1.981430 2.254954 2.525853 2.811645 #> w=0.96  1.16014395 1.523819 1.886168 1.981087 2.254142 2.524153 2.808396 #> w=0.965 1.16048655 1.523859 1.885932 1.980748 2.253339 2.522474 2.805132 #> w=0.97  1.16082566 1.523898 1.885698 1.980412 2.252545 2.520816 2.801975 #> w=0.975 1.16116134 1.523938 1.885467 1.980080 2.251760 2.519178 2.798849 #> w=0.98  1.16149363 1.523977 1.885238 1.979751 2.250983 2.517560 2.795763 #> w=0.985 1.16182259 1.524015 1.885011 1.979425 2.250215 2.515962 2.792717 #> w=0.99  1.16214827 1.524053 1.884787 1.979102 2.249455 2.514383 2.789713 #> w=0.995 1.16247072 1.524091 1.884564 1.978783 2.248704 2.512823 2.786748 #> w=1     1.16278998 1.524128 1.884344 1.978467 2.247960 2.511282 2.783823 #>            q0.99 #> w=0     5.951563 #> w=0.005 5.930313 #> w=0.01  5.909375 #> w=0.015 5.888736 #> w=0.02  5.868380 #> w=0.025 5.848294 #> w=0.03  5.828466 #> w=0.035 5.808883 #> w=0.04  5.789535 #> w=0.045 5.770412 #> w=0.05  5.751503 #> w=0.055 5.732798 #> w=0.06  5.714290 #> w=0.065 5.695970 #> w=0.07  5.677829 #> w=0.075 5.659861 #> w=0.08  5.642057 #> w=0.085 5.624411 #> w=0.09  5.606917 #> w=0.095 5.589568 #> w=0.1   5.572358 #> w=0.105 5.555282 #> w=0.11  5.538333 #> w=0.115 5.521523 #> w=0.12  5.504815 #> w=0.125 5.488220 #> w=0.13  5.471733 #> w=0.135 5.455349 #> w=0.14  5.439065 #> w=0.145 5.422877 #> w=0.15  5.406779 #> w=0.155 5.390770 #> w=0.16  5.374844 #> w=0.165 5.358999 #> w=0.17  5.343230 #> w=0.175 5.327535 #> w=0.18  5.311911 #> w=0.185 5.296353 #> w=0.19  5.280860 #> w=0.195 5.265428 #> w=0.2   5.250055 #> w=0.205 5.234737 #> w=0.21  5.219472 #> w=0.215 5.204257 #> w=0.22  5.189090 #> w=0.225 5.173969 #> w=0.23  5.158890 #> w=0.235 5.143853 #> w=0.24  5.128854 #> w=0.245 5.113891 #> w=0.25  5.098962 #> w=0.255 5.084065 #> w=0.26  5.069198 #> w=0.265 5.054358 #> w=0.27  5.039545 #> w=0.275 5.024756 #> w=0.28  5.009988 #> w=0.285 4.995241 #> w=0.29  4.980513 #> w=0.295 4.965801 #> w=0.3   4.951107 #> w=0.305 4.936431 #> w=0.31  4.921766 #> w=0.315 4.907110 #> w=0.32  4.892462 #> w=0.325 4.877760 #> w=0.33  4.863123 #> w=0.335 4.848490 #> w=0.34  4.833860 #> w=0.345 4.819230 #> w=0.35  4.804599 #> w=0.355 4.789967 #> w=0.36  4.775332 #> w=0.365 4.760692 #> w=0.37  4.746047 #> w=0.375 4.731395 #> w=0.38  4.716736 #> w=0.385 4.702067 #> w=0.39  4.687388 #> w=0.395 4.672699 #> w=0.4   4.657997 #> w=0.405 4.643282 #> w=0.41  4.628553 #> w=0.415 4.613809 #> w=0.42  4.599049 #> w=0.425 4.584273 #> w=0.43  4.569479 #> w=0.435 4.554668 #> w=0.44  4.539837 #> w=0.445 4.524987 #> w=0.45  4.510116 #> w=0.455 4.495225 #> w=0.46  4.480313 #> w=0.465 4.465373 #> w=0.47  4.450454 #> w=0.475 4.435459 #> w=0.48  4.420462 #> w=0.485 4.405437 #> w=0.49  4.390390 #> w=0.495 4.375320 #> w=0.5   4.360228 #> w=0.505 4.345114 #> w=0.51  4.329978 #> w=0.515 4.314820 #> w=0.52  4.299641 #> w=0.525 4.284442 #> w=0.53  4.269223 #> w=0.535 4.253986 #> w=0.54  4.238730 #> w=0.545 4.223457 #> w=0.55  4.208169 #> w=0.555 4.192866 #> w=0.56  4.177551 #> w=0.565 4.162225 #> w=0.57  4.146889 #> w=0.575 4.131546 #> w=0.58  4.116199 #> w=0.585 4.100848 #> w=0.59  4.085498 #> w=0.595 4.070175 #> w=0.6   4.054780 #> w=0.605 4.039454 #> w=0.61  4.024138 #> w=0.615 4.008836 #> w=0.62  3.993552 #> w=0.625 3.978291 #> w=0.63  3.963056 #> w=0.635 3.947851 #> w=0.64  3.932682 #> w=0.645 3.917553 #> w=0.65  3.902468 #> w=0.655 3.887433 #> w=0.66  3.872451 #> w=0.665 3.857530 #> w=0.67  3.842673 #> w=0.675 3.827860 #> w=0.68  3.813181 #> w=0.685 3.798535 #> w=0.69  3.783995 #> w=0.695 3.769540 #> w=0.7   3.755178 #> w=0.705 3.740912 #> w=0.71  3.726800 #> w=0.715 3.712731 #> w=0.72  3.698807 #> w=0.725 3.685007 #> w=0.73  3.671334 #> w=0.735 3.657798 #> w=0.74  3.644346 #> w=0.745 3.631082 #> w=0.75  3.617946 #> w=0.755 3.604956 #> w=0.76  3.592113 #> w=0.765 3.579423 #> w=0.77  3.566890 #> w=0.775 3.554518 #> w=0.78  3.542252 #> w=0.785 3.530196 #> w=0.79  3.518289 #> w=0.795 3.506543 #> w=0.8   3.494960 #> w=0.805 3.483540 #> w=0.81  3.472284 #> w=0.815 3.461192 #> w=0.82  3.450266 #> w=0.825 3.439502 #> w=0.83  3.428901 #> w=0.835 3.418462 #> w=0.84  3.408184 #> w=0.845 3.398067 #> w=0.85  3.388109 #> w=0.855 3.378309 #> w=0.86  3.368666 #> w=0.865 3.359177 #> w=0.87  3.349841 #> w=0.875 3.340656 #> w=0.88  3.331620 #> w=0.885 3.322732 #> w=0.89  3.313989 #> w=0.895 3.305389 #> w=0.9   3.296931 #> w=0.905 3.288611 #> w=0.91  3.280428 #> w=0.915 3.272376 #> w=0.92  3.264459 #> w=0.925 3.256672 #> w=0.93  3.249013 #> w=0.935 3.241479 #> w=0.94  3.234069 #> w=0.945 3.226780 #> w=0.95  3.219609 #> w=0.955 3.212555 #> w=0.96  3.205615 #> w=0.965 3.198788 #> w=0.97  3.192070 #> w=0.975 3.185460 #> w=0.98  3.178957 #> w=0.985 3.172557 #> w=0.99  3.166258 #> w=0.995 3.160059 #> w=1     3.153958"},{"path":"/reference/oc_bias.html","id":null,"dir":"Reference","previous_headings":"","what":"Assessing bias — oc_bias","title":"Assessing bias — oc_bias","text":"Assessment absolute bias posterior means medians given weight evidence level, using simulated data input.","code":""},{"path":"/reference/oc_bias.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assessing bias — oc_bias","text":"","code":"oc_bias(   m,   se,   true_effect,   weights = seq(0, 1, by = 0.01),   map_prior,   sigma,   n_cores = 1,   eval_strategy = \"sequential\" )"},{"path":"/reference/oc_bias.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assessing bias — oc_bias","text":"m Numerical vector simulated effect estimates. se Numerical vector simulated standard errors (m se need length). true_effect Numerical value, representing true treatment effect (usually mean simulated m). weights Vector weights informative component MAP prior (defaults seq(0, 1, = 0.01)). map_prior MAP prior containing information trials source population, created using RBesT; mixture normal distributions required. sigma Standard deviation weakly informative component MAP prior, recommended unit-information standard deviation. n_cores Integer value, representing number cores used (defaults 1); applies eval_strategy \"sequential\". eval_strategy Character variable, representing evaluation strategy, either \"sequential\", \"multisession\", \"multicore\" (see documentation future::plan, defaults \"sequential\").","code":""},{"path":"/reference/oc_bias.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assessing bias — oc_bias","text":"2-dimensional array containing results bias.","code":""},{"path":[]},{"path":"/reference/oc_bias.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assessing bias — oc_bias","text":"","code":"set.seed(123) n_sims <- 5 # small number for exemplary application  sim_dat <- list(   \"m\" = rnorm(n = n_sims, mean = 1.15, sd = 0.1),   \"se\" = rnorm(n = n_sims, mean = 1.8, sd = 0.3) ) results <- oc_bias(   m = sim_dat[[\"m\"]],   se = sim_dat[[\"se\"]],   true_effect = 1.15,   weights = seq(0, 1, by = 0.01),    map_prior = load_tipmap_data(\"tipmapPrior.rds\"),    sigma = 16.23,   eval_strategy = \"sequential\",   n_cores = 1 )  #> Warning: Quantile 0.5 possibly imprecise. #> Estimated precision= 4.31380082183221. #> Range = -3.202666058438 to 5.42493558522642 print(results) #>         oc_types #> weights  abs.bias.post.mean abs.bias.post.median #>   w=0           0.005204572          0.005204572 #>   w=0.01        0.031506884          0.079015454 #>   w=0.02        0.054389411          0.129263624 #>   w=0.03        0.074489107          0.164911837 #>   w=0.04        0.092291912          0.191278086 #>   w=0.05        0.108175481          0.211500574 #>   w=0.06        0.122438058          0.227468756 #>   w=0.07        0.135318499          0.240388033 #>   w=0.08        0.147010480          0.251038838 #>   w=0.09        0.157672788          0.259987130 #>   w=0.1         0.167436895          0.267602623 #>   w=0.11        0.176412633          0.274162009 #>   w=0.12        0.184692500          0.279870511 #>   w=0.13        0.192354971          0.284883488 #>   w=0.14        0.199467074          0.289320685 #>   w=0.15        0.206086416          0.293275866 #>   w=0.16        0.212262792          0.296835702 #>   w=0.17        0.218039468          0.300035662 #>   w=0.18        0.223454229          0.302936660 #>   w=0.19        0.228540217          0.305578716 #>   w=0.2         0.233326632          0.308000700 #>   w=0.21        0.237839299          0.310210655 #>   w=0.22        0.242101147          0.312257002 #>   w=0.23        0.246132603          0.314145972 #>   w=0.24        0.249951928          0.315893972 #>   w=0.25        0.253575494          0.317522267 #>   w=0.26        0.257018028          0.319054625 #>   w=0.27        0.260292807          0.320472195 #>   w=0.28        0.263411842          0.321794948 #>   w=0.29        0.266386015          0.323043331 #>   w=0.3         0.269225220          0.324217332 #>   w=0.31        0.271938464          0.325319098 #>   w=0.32        0.274533969          0.326360940 #>   w=0.33        0.277019256          0.327345640 #>   w=0.34        0.279401215          0.328277780 #>   w=0.35        0.281686175          0.329161462 #>   w=0.36        0.283879954          0.330000375 #>   w=0.37        0.285987916          0.330797841 #>   w=0.38        0.288015010          0.331556860 #>   w=0.39        0.289965811          0.332280151 #>   w=0.4         0.291844555          0.332970181 #>   w=0.41        0.293655169          0.333629196 #>   w=0.42        0.295401299          0.334259243 #>   w=0.43        0.297086336          0.334862193 #>   w=0.44        0.298713437          0.335439760 #>   w=0.45        0.300285543          0.335993516 #>   w=0.46        0.301805401          0.336524904 #>   w=0.47        0.303275577          0.337035255 #>   w=0.48        0.304698471          0.337525795 #>   w=0.49        0.306076332          0.337997656 #>   w=0.5         0.307411266          0.338451888 #>   w=0.51        0.308705252          0.338889460 #>   w=0.52        0.309960148          0.339311275 #>   w=0.53        0.311177700          0.339718168 #>   w=0.54        0.312359554          0.340110918 #>   w=0.55        0.313507259          0.340490252 #>   w=0.56        0.314622275          0.340856846 #>   w=0.57        0.315705981          0.341211331 #>   w=0.58        0.316759678          0.341554299 #>   w=0.59        0.317784598          0.341886302 #>   w=0.6         0.318781905          0.342207858 #>   w=0.61        0.319752701          0.342519453 #>   w=0.62        0.320698030          0.342821543 #>   w=0.63        0.321618883          0.343114558 #>   w=0.64        0.322516198          0.343398900 #>   w=0.65        0.323390867          0.343674949 #>   w=0.66        0.324243739          0.343943063 #>   w=0.67        0.325075617          0.344203580 #>   w=0.68        0.325887270          0.344456818 #>   w=0.69        0.326679426          0.344703078 #>   w=0.7         0.327452779          0.344942645 #>   w=0.71        0.328207993          0.345175789 #>   w=0.72        0.328945698          0.345402764 #>   w=0.73        0.329666496          0.345623813 #>   w=0.74        0.330370963          0.345839164 #>   w=0.75        0.331059648          0.346049036 #>   w=0.76        0.331733076          0.346241427 #>   w=0.77        0.332391748          0.346440949 #>   w=0.78        0.333036145          0.346635581 #>   w=0.79        0.333666725          0.346825500 #>   w=0.8         0.334283930          0.347010877 #>   w=0.81        0.334888180          0.347191872 #>   w=0.82        0.335479879          0.347368639 #>   w=0.83        0.336059415          0.347541324 #>   w=0.84        0.336627158          0.347710067 #>   w=0.85        0.337183466          0.347875002 #>   w=0.86        0.337728681          0.348036257 #>   w=0.87        0.338263131          0.348193952 #>   w=0.88        0.338787132          0.348348205 #>   w=0.89        0.339300988          0.348499128 #>   w=0.9         0.339804991          0.348646827 #>   w=0.91        0.340299421          0.348791404 #>   w=0.92        0.340784550          0.348932958 #>   w=0.93        0.341260637          0.349071581 #>   w=0.94        0.341727932          0.349207366 #>   w=0.95        0.342186678          0.349340397 #>   w=0.96        0.342637106          0.349470757 #>   w=0.97        0.343079441          0.349598527 #>   w=0.98        0.343513900          0.349723783 #>   w=0.99        0.343940690          0.349846598 #>   w=1           0.344360014          0.349966990"},{"path":"/reference/oc_coverage.html","id":null,"dir":"Reference","previous_headings":"","what":"Assessing coverage — oc_coverage","title":"Assessing coverage — oc_coverage","text":"Assessment coverage posterior intervals given weight evidence level, using simulated data input.","code":""},{"path":"/reference/oc_coverage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assessing coverage — oc_coverage","text":"","code":"oc_coverage(   m,   se,   true_effect,   weights = seq(0, 1, by = 0.01),   map_prior,   sigma,   n_cores = 1,   eval_strategy = \"sequential\" )"},{"path":"/reference/oc_coverage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assessing coverage — oc_coverage","text":"m Numerical vector simulated effect estimates. se Numerical vector simulated standard errors (m se need length). true_effect Numerical value, representing true treatment effect (usually mean simulated m). weights Vector weights informative component MAP prior (defaults seq(0, 1, = 0.01)). map_prior MAP prior containing information trials source population, created using RBesT; mixture normal distributions required. sigma Standard deviation weakly informative component MAP prior, recommended unit-information standard deviation. n_cores Integer value, representing number cores used (defaults 1); applies eval_strategy \"sequential\". eval_strategy Character variable, representing evaluation strategy, either \"sequential\", \"multisession\", \"multicore\" (see documentation future::plan, defaults \"sequential\").","code":""},{"path":"/reference/oc_coverage.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assessing coverage — oc_coverage","text":"2-dimensional array containing results coverage.","code":""},{"path":[]},{"path":"/reference/oc_coverage.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assessing coverage — oc_coverage","text":"","code":"set.seed(123) n_sims <- 5 # small number for exemplary application  sim_dat <- list(   \"m\" = rnorm(n = n_sims, mean = 1.15, sd = 0.1),   \"se\" = rnorm(n = n_sims, mean = 1.8, sd = 0.3) ) results <- oc_coverage(   m = sim_dat[[\"m\"]],   se = sim_dat[[\"se\"]],   true_effect = 1.15,   weights = seq(0, 1, by = 0.01),    map_prior = load_tipmap_data(\"tipmapPrior.rds\"),    sigma = 16.23 )  print(results) #>         oc_types #> weights  cov.50p cov.80p cov.90p cov.95p #>   w=0        1.0       1       1       1 #>   w=0.01     1.0       1       1       1 #>   w=0.02     1.0       1       1       1 #>   w=0.03     1.0       1       1       1 #>   w=0.04     1.0       1       1       1 #>   w=0.05     1.0       1       1       1 #>   w=0.06     1.0       1       1       1 #>   w=0.07     1.0       1       1       1 #>   w=0.08     1.0       1       1       1 #>   w=0.09     1.0       1       1       1 #>   w=0.1      1.0       1       1       1 #>   w=0.11     1.0       1       1       1 #>   w=0.12     1.0       1       1       1 #>   w=0.13     1.0       1       1       1 #>   w=0.14     1.0       1       1       1 #>   w=0.15     1.0       1       1       1 #>   w=0.16     1.0       1       1       1 #>   w=0.17     1.0       1       1       1 #>   w=0.18     1.0       1       1       1 #>   w=0.19     1.0       1       1       1 #>   w=0.2      1.0       1       1       1 #>   w=0.21     1.0       1       1       1 #>   w=0.22     1.0       1       1       1 #>   w=0.23     1.0       1       1       1 #>   w=0.24     1.0       1       1       1 #>   w=0.25     1.0       1       1       1 #>   w=0.26     1.0       1       1       1 #>   w=0.27     1.0       1       1       1 #>   w=0.28     1.0       1       1       1 #>   w=0.29     1.0       1       1       1 #>   w=0.3      1.0       1       1       1 #>   w=0.31     1.0       1       1       1 #>   w=0.32     1.0       1       1       1 #>   w=0.33     1.0       1       1       1 #>   w=0.34     1.0       1       1       1 #>   w=0.35     1.0       1       1       1 #>   w=0.36     1.0       1       1       1 #>   w=0.37     1.0       1       1       1 #>   w=0.38     1.0       1       1       1 #>   w=0.39     1.0       1       1       1 #>   w=0.4      1.0       1       1       1 #>   w=0.41     1.0       1       1       1 #>   w=0.42     1.0       1       1       1 #>   w=0.43     1.0       1       1       1 #>   w=0.44     1.0       1       1       1 #>   w=0.45     1.0       1       1       1 #>   w=0.46     1.0       1       1       1 #>   w=0.47     1.0       1       1       1 #>   w=0.48     1.0       1       1       1 #>   w=0.49     1.0       1       1       1 #>   w=0.5      1.0       1       1       1 #>   w=0.51     1.0       1       1       1 #>   w=0.52     1.0       1       1       1 #>   w=0.53     1.0       1       1       1 #>   w=0.54     1.0       1       1       1 #>   w=0.55     1.0       1       1       1 #>   w=0.56     1.0       1       1       1 #>   w=0.57     1.0       1       1       1 #>   w=0.58     1.0       1       1       1 #>   w=0.59     1.0       1       1       1 #>   w=0.6      1.0       1       1       1 #>   w=0.61     1.0       1       1       1 #>   w=0.62     1.0       1       1       1 #>   w=0.63     1.0       1       1       1 #>   w=0.64     1.0       1       1       1 #>   w=0.65     1.0       1       1       1 #>   w=0.66     1.0       1       1       1 #>   w=0.67     1.0       1       1       1 #>   w=0.68     1.0       1       1       1 #>   w=0.69     1.0       1       1       1 #>   w=0.7      1.0       1       1       1 #>   w=0.71     1.0       1       1       1 #>   w=0.72     1.0       1       1       1 #>   w=0.73     1.0       1       1       1 #>   w=0.74     1.0       1       1       1 #>   w=0.75     1.0       1       1       1 #>   w=0.76     1.0       1       1       1 #>   w=0.77     1.0       1       1       1 #>   w=0.78     1.0       1       1       1 #>   w=0.79     1.0       1       1       1 #>   w=0.8      1.0       1       1       1 #>   w=0.81     1.0       1       1       1 #>   w=0.82     1.0       1       1       1 #>   w=0.83     1.0       1       1       1 #>   w=0.84     1.0       1       1       1 #>   w=0.85     0.8       1       1       1 #>   w=0.86     0.8       1       1       1 #>   w=0.87     0.8       1       1       1 #>   w=0.88     0.8       1       1       1 #>   w=0.89     0.8       1       1       1 #>   w=0.9      0.8       1       1       1 #>   w=0.91     0.8       1       1       1 #>   w=0.92     0.8       1       1       1 #>   w=0.93     0.8       1       1       1 #>   w=0.94     0.8       1       1       1 #>   w=0.95     0.8       1       1       1 #>   w=0.96     0.8       1       1       1 #>   w=0.97     0.8       1       1       1 #>   w=0.98     0.8       1       1       1 #>   w=0.99     0.8       1       1       1 #>   w=1        0.8       1       1       1"},{"path":"/reference/oc_pos.html","id":null,"dir":"Reference","previous_headings":"","what":"Assessing probability of success — oc_pos","title":"Assessing probability of success — oc_pos","text":"Assessment probability truly falsely (depending simulated scenario) rejecting null hypothesis interest given weight evidence level, using simulated data input.","code":""},{"path":"/reference/oc_pos.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Assessing probability of success — oc_pos","text":"","code":"oc_pos(   m,   se,   probs,   weights = seq(0, 1, by = 0.01),   map_prior,   sigma,   null_effect = 0,   direction_pos = T,   n_cores = 1,   eval_strategy = \"sequential\" )"},{"path":"/reference/oc_pos.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Assessing probability of success — oc_pos","text":"m Numerical vector simulated effect estimates. se Numerical vector simulated standard errors (m se need length). probs Vector quantiles q, 1 minus q representing evidence level interest (positive effect estimate indicate beneficial treatment). weights Vector weights informative component MAP prior (defaults seq(0, 1, = 0.01)). map_prior MAP prior containing information trials source population, created using RBesT; mixture normal distributions required. sigma Standard deviation weakly informative component MAP prior, recommended unit-information standard deviation. null_effect Numerical value, representing null effect (defaults 0). direction_pos Logical value, TRUE (default) effects greater null_effect indicate beneficial treatment FALSE otherwise. n_cores Integer value, representing number cores used (defaults 1); applies eval_strategy \"sequential\". eval_strategy Character variable, representing evaluation strategy, either \"sequential\", \"multisession\", \"multicore\" (see documentation future::plan, defaults \"sequential\").","code":""},{"path":"/reference/oc_pos.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Assessing probability of success — oc_pos","text":"2-dimensional array containing probabilities, either truly (probability success) falsely rejecting null hypothesis interest given weight evidence level.","code":""},{"path":[]},{"path":"/reference/oc_pos.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Assessing probability of success — oc_pos","text":"","code":"set.seed(123) n_sims <- 5 # small number for exemplary application sim_dat <- list(   \"m\" = rnorm(n = n_sims, mean = 1.15, sd = 0.1),   \"se\" = rnorm(n = n_sims, mean = 1.8, sd = 0.3) ) results <- oc_pos(   m = sim_dat[[\"m\"]],   se = sim_dat[[\"se\"]],   probs = c(0.025, 0.05, 0.1, 0.2),    weights = seq(0, 1, by = 0.01),    map_prior = load_tipmap_data(\"tipmapPrior.rds\"),    sigma = 16.23,   null_effect = 0,   direction_pos = TRUE,    eval_strategy = \"sequential\",   n_cores = 1 )  print(results) #>        q=0.025 q=0.05 q=0.1 q=0.2 #> w=0        0.0    0.0   0.0   0.2 #> w=0.01     0.0    0.0   0.0   0.2 #> w=0.02     0.0    0.0   0.0   0.4 #> w=0.03     0.0    0.0   0.0   0.6 #> w=0.04     0.0    0.0   0.0   0.6 #> w=0.05     0.0    0.0   0.0   0.6 #> w=0.06     0.0    0.0   0.0   0.8 #> w=0.07     0.0    0.0   0.0   0.8 #> w=0.08     0.0    0.0   0.2   0.8 #> w=0.09     0.0    0.0   0.2   1.0 #> w=0.1      0.0    0.0   0.2   1.0 #> w=0.11     0.0    0.0   0.2   1.0 #> w=0.12     0.0    0.0   0.2   1.0 #> w=0.13     0.0    0.0   0.2   1.0 #> w=0.14     0.0    0.0   0.2   1.0 #> w=0.15     0.0    0.0   0.4   1.0 #> w=0.16     0.0    0.0   0.6   1.0 #> w=0.17     0.0    0.0   0.6   1.0 #> w=0.18     0.0    0.0   0.6   1.0 #> w=0.19     0.0    0.0   0.6   1.0 #> w=0.2      0.0    0.0   0.6   1.0 #> w=0.21     0.0    0.0   0.6   1.0 #> w=0.22     0.0    0.0   0.8   1.0 #> w=0.23     0.0    0.0   0.8   1.0 #> w=0.24     0.0    0.2   0.8   1.0 #> w=0.25     0.0    0.2   0.8   1.0 #> w=0.26     0.0    0.2   0.8   1.0 #> w=0.27     0.0    0.2   0.8   1.0 #> w=0.28     0.0    0.2   1.0   1.0 #> w=0.29     0.0    0.2   1.0   1.0 #> w=0.3      0.0    0.2   1.0   1.0 #> w=0.31     0.0    0.2   1.0   1.0 #> w=0.32     0.0    0.2   1.0   1.0 #> w=0.33     0.0    0.2   1.0   1.0 #> w=0.34     0.0    0.2   1.0   1.0 #> w=0.35     0.0    0.4   1.0   1.0 #> w=0.36     0.0    0.4   1.0   1.0 #> w=0.37     0.0    0.6   1.0   1.0 #> w=0.38     0.0    0.6   1.0   1.0 #> w=0.39     0.0    0.6   1.0   1.0 #> w=0.4      0.0    0.6   1.0   1.0 #> w=0.41     0.0    0.6   1.0   1.0 #> w=0.42     0.0    0.6   1.0   1.0 #> w=0.43     0.0    0.6   1.0   1.0 #> w=0.44     0.0    0.6   1.0   1.0 #> w=0.45     0.0    0.6   1.0   1.0 #> w=0.46     0.0    0.8   1.0   1.0 #> w=0.47     0.0    0.8   1.0   1.0 #> w=0.48     0.0    0.8   1.0   1.0 #> w=0.49     0.0    0.8   1.0   1.0 #> w=0.5      0.2    0.8   1.0   1.0 #> w=0.51     0.2    0.8   1.0   1.0 #> w=0.52     0.2    0.8   1.0   1.0 #> w=0.53     0.2    0.8   1.0   1.0 #> w=0.54     0.2    1.0   1.0   1.0 #> w=0.55     0.2    1.0   1.0   1.0 #> w=0.56     0.2    1.0   1.0   1.0 #> w=0.57     0.2    1.0   1.0   1.0 #> w=0.58     0.2    1.0   1.0   1.0 #> w=0.59     0.2    1.0   1.0   1.0 #> w=0.6      0.2    1.0   1.0   1.0 #> w=0.61     0.2    1.0   1.0   1.0 #> w=0.62     0.2    1.0   1.0   1.0 #> w=0.63     0.2    1.0   1.0   1.0 #> w=0.64     0.4    1.0   1.0   1.0 #> w=0.65     0.4    1.0   1.0   1.0 #> w=0.66     0.4    1.0   1.0   1.0 #> w=0.67     0.6    1.0   1.0   1.0 #> w=0.68     0.6    1.0   1.0   1.0 #> w=0.69     0.6    1.0   1.0   1.0 #> w=0.7      0.6    1.0   1.0   1.0 #> w=0.71     0.6    1.0   1.0   1.0 #> w=0.72     0.6    1.0   1.0   1.0 #> w=0.73     0.6    1.0   1.0   1.0 #> w=0.74     0.6    1.0   1.0   1.0 #> w=0.75     0.8    1.0   1.0   1.0 #> w=0.76     0.8    1.0   1.0   1.0 #> w=0.77     0.8    1.0   1.0   1.0 #> w=0.78     0.8    1.0   1.0   1.0 #> w=0.79     0.8    1.0   1.0   1.0 #> w=0.8      0.8    1.0   1.0   1.0 #> w=0.81     0.8    1.0   1.0   1.0 #> w=0.82     1.0    1.0   1.0   1.0 #> w=0.83     1.0    1.0   1.0   1.0 #> w=0.84     1.0    1.0   1.0   1.0 #> w=0.85     1.0    1.0   1.0   1.0 #> w=0.86     1.0    1.0   1.0   1.0 #> w=0.87     1.0    1.0   1.0   1.0 #> w=0.88     1.0    1.0   1.0   1.0 #> w=0.89     1.0    1.0   1.0   1.0 #> w=0.9      1.0    1.0   1.0   1.0 #> w=0.91     1.0    1.0   1.0   1.0 #> w=0.92     1.0    1.0   1.0   1.0 #> w=0.93     1.0    1.0   1.0   1.0 #> w=0.94     1.0    1.0   1.0   1.0 #> w=0.95     1.0    1.0   1.0   1.0 #> w=0.96     1.0    1.0   1.0   1.0 #> w=0.97     1.0    1.0   1.0   1.0 #> w=0.98     1.0    1.0   1.0   1.0 #> w=0.99     1.0    1.0   1.0   1.0 #> w=1        1.0    1.0   1.0   1.0"},{"path":"/reference/tipmap-package.html","id":null,"dir":"Reference","previous_headings":"","what":"tipmap: Tipping Point Analysis for Bayesian Dynamic Borrowing — tipmap-package","title":"tipmap: Tipping Point Analysis for Bayesian Dynamic Borrowing — tipmap-package","text":"Tipping point analysis clinical trials employ Bayesian dynamic borrowing via robust meta-analytic predictive (MAP) priors. functions facilitate expert elicitation primary weight informative component robust MAP prior computation operating characteristics. Intended use planning, analysis interpretation extrapolation studies pediatric drug development, applicability generally wider.","code":""},{"path":[]},{"path":"/reference/tipmap-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"tipmap: Tipping Point Analysis for Bayesian Dynamic Borrowing — tipmap-package","text":"Maintainer: Christian Stock christian.stock@boehringer-ingelheim.com (ORCID) Authors: Morten Dreher contributors: Emma Torrini [contributor] Boehringer Ingelheim Pharma GmbH & Co. KG [copyright holder, funder]","code":""},{"path":"/reference/tipmap_darkblue.html","id":null,"dir":"Reference","previous_headings":"","what":"Custom dark blue — tipmap_darkblue","title":"Custom dark blue — tipmap_darkblue","text":"Custom dark blue","code":""},{"path":"/reference/tipmap_darkblue.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Custom dark blue — tipmap_darkblue","text":"","code":"tipmap_darkblue"},{"path":"/reference/tipmap_darkblue.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Custom dark blue — tipmap_darkblue","text":"object class character length 1.","code":""},{"path":"/reference/tipmap_lightred.html","id":null,"dir":"Reference","previous_headings":"","what":"Custom light red — tipmap_lightred","title":"Custom light red — tipmap_lightred","text":"Custom light red","code":""},{"path":"/reference/tipmap_lightred.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Custom light red — tipmap_lightred","text":"","code":"tipmap_lightred"},{"path":"/reference/tipmap_lightred.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Custom light red — tipmap_lightred","text":"object class character length 1.","code":""},{"path":"/reference/tipmap_plot.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize tipping point analysis — tipmap_plot","title":"Visualize tipping point analysis — tipmap_plot","text":"Uses data frame created create_tipmap_data() visualize tipping point analysis.","code":""},{"path":"/reference/tipmap_plot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize tipping point analysis — tipmap_plot","text":"","code":"tipmap_plot(   tipmap_data,   target_pop_lab = \"Trial in target\\n population\",   y_range = NULL,   y_breaks = NULL,   title = NULL,   y_lab = \"Mean difference\",   x_lab = \"Weight on informative component of robust MAP prior\",   map_prior_lab = \"MAP\\nprior\",   meta_analysis_lab = \"MA\",   legend_title = \"Posterior quantile\",   null_effect = 0 )"},{"path":"/reference/tipmap_plot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize tipping point analysis — tipmap_plot","text":"tipmap_data data frame containing tipping point data, generated [create_tipmap_data()]. target_pop_lab label trial target population. y_range optional argument specifying range y-axis. y_breaks optional vector specifying breaks y-axis. title plot title. y_lab label y axis. Defaults \"Mean difference\". x_lab label x axis. Defaults \"Weight informative component MAP prior\". map_prior_lab label MAP prior. Defaults \"MAP prior\" meta_analysis_lab optional label meta-analysis (included). legend_title optional title plot legend. Defaults \"Posterior quantiles\". null_effect null treatment effect, determining tipping points calculated. Defaults 0.","code":""},{"path":"/reference/tipmap_plot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize tipping point analysis — tipmap_plot","text":"ggplot object tipping point plot","code":""},{"path":[]},{"path":"/reference/tipmap_plot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize tipping point analysis — tipmap_plot","text":"","code":"tipmap_data <- load_tipmap_data(\"tipdat.rds\") tipmap_plot(tipmap_data) #> Warning: All aesthetics have length 1, but the data has 201 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: All aesthetics have length 1, but the data has 201 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: All aesthetics have length 1, but the data has 201 rows. #> ℹ Did you mean to use `annotate()`? #> Warning: All aesthetics have length 1, but the data has 201 rows. #> ℹ Did you mean to use `annotate()`?"},{"path":"/news/index.html","id":"tipmap-053","dir":"Changelog","previous_headings":"","what":"tipmap 0.5.3","title":"tipmap 0.5.3","text":"Adding option highest posterior density intervals create_posterior_data().","code":""},{"path":"/news/index.html","id":"tipmap-052","dir":"Changelog","previous_headings":"","what":"tipmap 0.5.2","title":"tipmap 0.5.2","text":"CRAN release: 2023-08-14 Fix comply CRAN requirements","code":""},{"path":"/news/index.html","id":"tipmap-051","dir":"Changelog","previous_headings":"","what":"tipmap 0.5.1","title":"tipmap 0.5.1","text":"CRAN release: 2023-08-13 Functionality added compute operating characteristics (new functions : oc_pos(), oc_bias() oc_coverage()) Use assertthat::assert_that() check function inputs","code":""},{"path":"/news/index.html","id":"tipmap-042","dir":"Changelog","previous_headings":"","what":"tipmap 0.4.2","title":"tipmap 0.4.2","text":"CRAN release: 2023-06-17 GitHub repository changed Modified tipmap_plot() avoid problems tipping points 0 1 Obsolete function get_stochast_weight_posterior() removed Modifications documentation vignettes","code":""},{"path":"/news/index.html","id":"tipmap-041","dir":"Changelog","previous_headings":"","what":"tipmap 0.4.1","title":"tipmap 0.4.1","text":"CRAN release: 2023-04-24 New vignette weight determination via expert elicitation Updated README file Modifications documentation","code":""},{"path":"/news/index.html","id":"tipmap-039","dir":"Changelog","previous_headings":"","what":"tipmap 0.3.9","title":"tipmap 0.3.9","text":"CRAN release: 2022-12-07 Modifications computation posterior stochastic weights Modifications introductory vignette","code":""},{"path":"/news/index.html","id":"tipmap-037","dir":"Changelog","previous_headings":"","what":"tipmap 0.3.7","title":"tipmap 0.3.7","text":"CRAN release: 2022-10-23 Introductory vignette added Modification Readme file","code":""},{"path":"/news/index.html","id":"tipmap-035","dir":"Changelog","previous_headings":"","what":"tipmap 0.3.5","title":"tipmap 0.3.5","text":"CRAN release: 2022-09-02 Addition functions facilitating expert elicitation weights Renaming functions variable names comply code style guidelines Updates documentation","code":""},{"path":"/news/index.html","id":"tipmap-017","dir":"Changelog","previous_headings":"","what":"tipmap 0.1.7","title":"tipmap 0.1.7","text":"CRAN release: 2022-07-12 Reduced functionality due M1mac issue imported function Updates documentation","code":""},{"path":"/news/index.html","id":"tipmap-011","dir":"Changelog","previous_headings":"","what":"tipmap 0.1.1","title":"tipmap 0.1.1","text":"CRAN release: 2022-07-01 Initial CRAN submission","code":""}]
